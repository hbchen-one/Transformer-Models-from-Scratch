{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Encoder_only_transformer_AG_News_classification.ipynb",
   "provenance": [
    {
     "file_id": "1P7oU2EWQ1Qk17N9NutZv9FV1a_hMGUoR",
     "timestamp": 1647373127550
    }
   ],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder-only transformer model for AG News classification"
   ],
   "metadata": {
    "id": "jtwTIjo7oOuB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, I train a encoder-only transformer to do text classification on the AG_NEWS dataset.\n",
    "Text classification seems to be a pretty simple task, and using transformer is probably overkill. But this is my first time implementing the transformer structure from scratch (including the self-attention layers), and it was fun :-)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "o5eW3azllzRd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# some commands in th is notebook require torchtext 0.12.0\n",
    "!pip install  torchtext --upgrade --quiet\n",
    "!pip install torchdata --quiet\n",
    "!pip install torchinfo --quiet"
   ],
   "metadata": {
    "id": "Mtq3abS2lL_i",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854674594,
     "user_tz": 240,
     "elapsed": 3522,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torchtext\n",
    "import torchdata\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchinfo\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ],
   "metadata": {
    "id": "bQEBuaIm5s9R",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854702225,
     "user_tz": 240,
     "elapsed": 916,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "45a2a23d-8c96-4e5a-e448-a456a9f32702"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data processing"
   ],
   "metadata": {
    "id": "SmM0VcYnCC8M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# One can easily modify the data processing part of this code to accommodate for   other datasets for text classification listed in https://pytorch.org/text/stable/datasets.html#text-classification\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "num_classes = len(set([label for (label, text) in train_iter]))\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ],
   "metadata": {
    "id": "7UdYsN6J5uke",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854711979,
     "user_tz": 240,
     "elapsed": 7185,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# see an example of the dateset\n",
    "next(iter(train_iter))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HchVgEXWlqLz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854711980,
     "user_tz": 240,
     "elapsed": 11,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "e666aa95-bceb-4b92-9ae6-38e91f76a48b"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:181: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# convert the labels to be in range(0, num_classes)\n",
    "y_train = torch.tensor([label-1 for (label, text) in train_iter])\n",
    "y_test  = torch.tensor([label-1 for (label, text) in test_iter])\n",
    "\n",
    "# There are many \"\\\\\" in the texts in the AG_news dataset, we get rid of them.\n",
    "train_iter = ((label, text.replace(\"\\\\\", \" \")) for label, text in train_iter)\n",
    "test_iter  = ((label, text.replace(\"\\\\\", \" \")) for label, text in test_iter)\n",
    "\n",
    "# tokenize the texts, and truncate the number of words in each text to max_seq_len\n",
    "max_seq_len = 100\n",
    "x_train_texts = [tokenizer(text.lower())[0:max_seq_len]\n",
    "                 for (label, text) in train_iter]\n",
    "x_test_texts  = [tokenizer(text.lower())[0:max_seq_len]\n",
    "                 for (label, text) in test_iter]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DeTWUptkYnG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854724343,
     "user_tz": 240,
     "elapsed": 12259,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "ce97c430-3422-48b2-f879-bc8bdd0688b2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# build the vocabulary and word-to-integer map\n",
    "counter = collections.Counter()\n",
    "for text in x_train_texts:\n",
    "    counter.update(text)\n",
    "\n",
    "vocab_size = 15000\n",
    "most_common_words = np.array(counter.most_common(vocab_size - 2))\n",
    "vocab = most_common_words[:,0]\n",
    "\n",
    "# indexes for the padding token, and unknown tokens\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "word_to_id = {vocab[i]: i + 2 for i in range(len(vocab))}"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MYVE8HSGkYnH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854725747,
     "user_tz": 240,
     "elapsed": 1406,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# map the words in the training and test texts to integers\n",
    "x_train = [torch.tensor([word_to_id.get(word, UNK) for word in text])\n",
    "           for text in x_train_texts]\n",
    "x_test  = [torch.tensor([word_to_id.get(word, UNK) for word in text])\n",
    "          for text in x_test_texts]\n",
    "x_test = torch.nn.utils.rnn.pad_sequence(x_test,\n",
    "                                batch_first=True, padding_value = PAD)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "I7-4KQI8kYnH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854730011,
     "user_tz": 240,
     "elapsed": 3129,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# constructing the dataset in order to be compatible with torch.utils.data.Dataloader\n",
    "class AGNewsDataset:\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.features[item], self.labels[item]\n",
    "\n",
    "\n",
    "train_dataset = AGNewsDataset(x_train, y_train)\n",
    "test_dataset  = AGNewsDataset(x_test, y_test)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hJe8LAUNkYnI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854730012,
     "user_tz": 240,
     "elapsed": 2,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# collate_fn to be used in torch.utils.data.DataLoader().\n",
    "# It pads the texts in each batch such that they have the same sequence length.\n",
    "def pad_sequence(batch):\n",
    "    texts  = [text for text, label in batch]\n",
    "    labels = torch.tensor([label for text, label in batch])\n",
    "    texts_padded = torch.nn.utils.rnn.pad_sequence(texts,\n",
    "                                batch_first=True, padding_value = PAD)\n",
    "    return texts_padded, labels\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "                        collate_fn = pad_sequence)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True,\n",
    "                        collate_fn = pad_sequence)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ov3tX4sRkYnI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854730626,
     "user_tz": 240,
     "elapsed": 101,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the encoder-only transformer model for text classification"
   ],
   "metadata": {
    "id": "zNVoCKz0CM3g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformer_blocks import Encoder\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.linear = nn.Linear(config.d_embed, num_classes)\n",
    "\n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.encoder(x, pad_mask)\n",
    "        return  self.linear(torch.mean(x,-2))"
   ],
   "metadata": {
    "id": "gxqdJRLXyHin",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650855020691,
     "user_tz": 240,
     "elapsed": 95,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    encoder_vocab_size: int\n",
    "    d_embed: int\n",
    "    # d_ff is the dimension of the fully-connected  feed-forward layer\n",
    "    d_ff: int\n",
    "    # h is the number of attention head\n",
    "    h: int\n",
    "    N_encoder: int\n",
    "    max_seq_len: int\n",
    "    dropout: float\n",
    "\n",
    "def make_model(config):\n",
    "    model = Transformer(config, num_classes).to(DEVICE)\n",
    "    # initialize model parameters\n",
    "    # it seems that this initialization is very important!\n",
    "    for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    return model"
   ],
   "metadata": {
    "id": "gxYYnrC-FAgI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854802934,
     "user_tz": 240,
     "elapsed": 98,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model"
   ],
   "metadata": {
    "id": "1HdgZPJBoKY2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader):\n",
    "    model.train()\n",
    "    losses, acc, count = [], 0, 0\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for idx, (x, y)  in  pbar:\n",
    "        optimizer.zero_grad()\n",
    "        features= x.to(DEVICE)\n",
    "        labels  = y.to(DEVICE)\n",
    "        pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))\n",
    "        pred = model(features, pad_mask)\n",
    "\n",
    "        loss = loss_fn(pred, labels).to(DEVICE)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        acc += (pred.argmax(1) == labels).sum().item()\n",
    "        count += len(labels)\n",
    "        # report progress\n",
    "        if idx>0 and idx%50 == 0:\n",
    "            pbar.set_description(f'train loss={loss.item():.4f}, train_acc={acc/count:.4f}')\n",
    "    return np.mean(losses), acc/count\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs):\n",
    "    for ep in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, test_loader)\n",
    "        print(f'ep {ep}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}')\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            features = x_test.to(DEVICE)\n",
    "            labels  = y_test.to(DEVICE)\n",
    "            pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))\n",
    "            pred = model(features, pad_mask)\n",
    "            loss = loss_fn(pred,labels).to(DEVICE)\n",
    "            losses.append(loss.item())\n",
    "            acc = (pred.argmax(1) == labels).sum().item()\n",
    "            count = len(labels)\n",
    "    return np.mean(losses), acc/count"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ydp6IfBrkYnL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854878396,
     "user_tz": 240,
     "elapsed": 93,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "Transformer                                        --\n",
      "├─Encoder: 1-1                                     --\n",
      "│    └─Embedding: 2-1                              480,000\n",
      "│    └─ModuleList: 2-2                             --\n",
      "│    │    └─EncoderBlock: 3-1                      12,704\n",
      "│    └─Dropout: 2-3                                --\n",
      "│    └─LayerNorm: 2-4                              64\n",
      "├─Linear: 1-2                                      132\n",
      "===========================================================================\n",
      "Total params: 492,900\n",
      "Trainable params: 492,900\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(encoder_vocab_size = vocab_size,\n",
    "                     d_embed = 32,\n",
    "                     d_ff = 4*32,\n",
    "                     h = 1,\n",
    "                     N_encoder = 1,\n",
    "                     max_seq_len = max_seq_len,\n",
    "                     dropout = 0.1\n",
    "                     )\n",
    "model = make_model(config)\n",
    "print(torchinfo.summary(model))\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIau66WRlzRm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854855057,
     "user_tz": 240,
     "elapsed": 100,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "d1b968b0-2d46-492e-e765-9cfc4a4f13ad"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train loss=0.1355, train_acc=0.9520: 100%|██████████| 938/938 [00:08<00:00, 112.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ep 0: val_loss=0.2625, val_acc=0.9180\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train loss=0.1171, train_acc=0.9602: 100%|██████████| 938/938 [00:08<00:00, 114.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ep 1: val_loss=0.2974, val_acc=0.9158\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train loss=0.0689, train_acc=0.9672: 100%|██████████| 938/938 [00:08<00:00, 114.47it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ep 2: val_loss=0.3249, val_acc=0.9163\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train loss=0.0443, train_acc=0.9725: 100%|██████████| 938/938 [00:08<00:00, 114.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ep 3: val_loss=0.3455, val_acc=0.9122\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train loss=0.1271, train_acc=0.9764: 100%|██████████| 938/938 [00:08<00:00, 114.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ep 4: val_loss=0.3807, val_acc=0.9105\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, epochs=5)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qo7RYNx0lzRm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854934525,
     "user_tz": 240,
     "elapsed": 54045,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "ee2284c5-5d1d-49e0-f14d-fb15111b0288"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## text classification example"
   ],
   "metadata": {
    "id": "XBSr0A9noh2i"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is a Sci/Tec news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def classify_news(news):\n",
    "    x_text = tokenizer(news.lower())[0:max_seq_len]\n",
    "    x_int = torch.tensor([[word_to_id.get(word, UNK) for word in x_text]]).to(DEVICE)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_int).argmax(1).item() + 1\n",
    "    print(f\"This is a {ag_news_label[pred]} news\")\n",
    "\n",
    "# The model correctly classifies a theoretical physics news as Sci/Tec news, :-)\n",
    "news = \"\"\"The conformal bootstrapDavid Poland1,2and David Simmons-Duﬃn2*The conformal bootstrap was\n",
    "proposed in the 1970s as a strategy for calculating the properties of second-order phasetransitions.\n",
    "After spectacular success elucidating two-dimensional systems, little progress was made on systems in\n",
    " higher dimensions until a recent renaissance beginning in 2008. We report on some of the main results and\n",
    "  ideas from thisrenaissance, focusing on new determinations of critical exponents and correlation\n",
    "  functions in the three-dimensional Ising and O(N) models.\n",
    "\"\"\"\n",
    "classify_news(news)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaAPcoPTkYnM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650854946744,
     "user_tz": 240,
     "elapsed": 2,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "26790595-6238-41ad-e491-14fc9493bee1"
   }
  }
 ]
}