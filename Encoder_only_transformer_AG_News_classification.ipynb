{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Encoder_only_transformer_AG_News_classification.ipynb",
   "provenance": [
    {
     "file_id": "1P7oU2EWQ1Qk17N9NutZv9FV1a_hMGUoR",
     "timestamp": 1647373127550
    }
   ],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder-only transformer model for AG News classification"
   ],
   "metadata": {
    "id": "jtwTIjo7oOuB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, I train a encoder-only transformer to do text classification on the AG_NEWS dataset.\n",
    "Text classification seems to be a pretty simple task, and using transformer is probably overkill. But this is my first time implementing the transformer structure from scratch (including the self-attention module), and it was fun :-)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "o5eW3azllzRd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# some commands in th is notebook require torchtext 0.12.0\n",
    "# !pip install  torchtext --upgrade --quiet\n",
    "# !pip install torchdata --quiet\n",
    "# !pip install torchinfo --quiet"
   ],
   "metadata": {
    "id": "Mtq3abS2lL_i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torchtext\n",
    "import torchdata\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchinfo\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ],
   "metadata": {
    "id": "bQEBuaIm5s9R",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650855874311,
     "user_tz": 240,
     "elapsed": 774,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fddbd5b7-2e54-4e94-a047-40571ee7508c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data processing"
   ],
   "metadata": {
    "id": "SmM0VcYnCC8M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# One can easily modify the data processing part of this code to accommodate for   other datasets for text classification listed in https://pytorch.org/text/stable/datasets.html#text-classification\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "num_classes = len(set([label for (label, text) in train_iter]))\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ],
   "metadata": {
    "id": "7UdYsN6J5uke",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650856391708,
     "user_tz": 240,
     "elapsed": 3725,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# see an example of the dateset\n",
    "next(iter(train_iter))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HchVgEXWlqLz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650856391708,
     "user_tz": 240,
     "elapsed": 4,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "a31a2646-4b3a-42ab-aa45-34196764d03d"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert the labels to be in range(0, num_classes)\n",
    "y_train = torch.tensor([label-1 for (label, text) in train_iter])\n",
    "y_test  = torch.tensor([label-1 for (label, text) in test_iter])\n",
    "\n",
    "# There are many \"\\\\\" in the texts in the AG_news dataset, we get rid of them.\n",
    "train_iter = ((label, text.replace(\"\\\\\", \" \")) for label, text in train_iter)\n",
    "test_iter  = ((label, text.replace(\"\\\\\", \" \")) for label, text in test_iter)\n",
    "\n",
    "# tokenize the texts, and truncate the number of words in each text to max_seq_len\n",
    "max_seq_len = 100\n",
    "x_train_texts = [tokenizer(text.lower())[0:max_seq_len]\n",
    "                 for (label, text) in train_iter]\n",
    "x_test_texts  = [tokenizer(text.lower())[0:max_seq_len]\n",
    "                 for (label, text) in test_iter]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6DeTWUptkYnG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build the vocabulary and word-to-integer map\n",
    "counter = collections.Counter()\n",
    "for text in x_train_texts:\n",
    "    counter.update(text)\n",
    "\n",
    "vocab_size = 15000\n",
    "most_common_words = np.array(counter.most_common(vocab_size - 2))\n",
    "vocab = most_common_words[:,0]\n",
    "\n",
    "# indexes for the padding token, and unknown tokens\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "word_to_id = {vocab[i]: i + 2 for i in range(len(vocab))}"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MYVE8HSGkYnH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# map the words in the training and test texts to integers\n",
    "x_train = [torch.tensor([word_to_id.get(word, UNK) for word in text])\n",
    "           for text in x_train_texts]\n",
    "x_test  = [torch.tensor([word_to_id.get(word, UNK) for word in text])\n",
    "          for text in x_test_texts]\n",
    "x_test = torch.nn.utils.rnn.pad_sequence(x_test,\n",
    "                                batch_first=True, padding_value = PAD)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "I7-4KQI8kYnH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# constructing the dataset in order to be compatible with torch.utils.data.Dataloader\n",
    "class AGNewsDataset:\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.features[item], self.labels[item]\n",
    "\n",
    "\n",
    "train_dataset = AGNewsDataset(x_train, y_train)\n",
    "test_dataset  = AGNewsDataset(x_test, y_test)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hJe8LAUNkYnI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# collate_fn to be used in torch.utils.data.DataLoader().\n",
    "# It pads the texts in each batch such that they have the same sequence length.\n",
    "def pad_sequence(batch):\n",
    "    texts  = [text for text, label in batch]\n",
    "    labels = torch.tensor([label for text, label in batch])\n",
    "    texts_padded = torch.nn.utils.rnn.pad_sequence(texts,\n",
    "                                batch_first=True, padding_value = PAD)\n",
    "    return texts_padded, labels\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "                        collate_fn = pad_sequence)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True,\n",
    "                        collate_fn = pad_sequence)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ov3tX4sRkYnI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the encoder-only transformer model for text classification"
   ],
   "metadata": {
    "id": "zNVoCKz0CM3g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from transformer_blocks import Encoder\n",
    "# One can also import Encoder from transformer_blocks.py in my Github repository.\n",
    "# I copied the code here so that this notebook is self-contained.  \n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_embed, dropout=0.0):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_embed % h == 0 # check the h number\n",
    "        self.d_k = d_embed//h\n",
    "        self.d_embed = d_embed\n",
    "        self.h = h\n",
    "        self.WQ = nn.Linear(d_embed, d_embed)\n",
    "        self.WK = nn.Linear(d_embed, d_embed)\n",
    "        self.WV = nn.Linear(d_embed, d_embed)\n",
    "        self.linear = nn.Linear(d_embed, d_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_query, x_key, x_value, mask=None):\n",
    "        nbatch = x_query.size(0) # get batch size\n",
    "        # 1) Linear projections to get the multi-head query, key and value tensors\n",
    "        # x_query, x_key, x_value dimension: nbatch * seq_len * d_embed\n",
    "        # LHS query, key, value dimensions: nbatch * h * seq_len * d_k\n",
    "        query = self.WQ(x_query).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
    "        key   = self.WK(x_key).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
    "        value = self.WV(x_value).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
    "        # 2) Attention\n",
    "        # scores has dimensions: nbatch * h * seq_len * seq_len\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.d_embed)\n",
    "        # 3) Mask out padding tokens and future tokens\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        # p_atten dimensions: nbatch * h * seq_len * seq_len\n",
    "        p_atten = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        p_atten = self.dropout(p_atten)\n",
    "        # x dimensions: nbatch * h * seq_len * d_k\n",
    "        x = torch.matmul(p_atten, value)\n",
    "        # x now has dimensions:nbtach * seq_len * d_embed\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatch, -1, self.d_embed)\n",
    "        return self.linear(x) # final linear layer\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "  '''residual connection: x + dropout(sublayer(layernorm(x))) '''\n",
    "  def __init__(self, dim, dropout):\n",
    "      super().__init__()\n",
    "      self.drop = nn.Dropout(dropout)\n",
    "      self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "  def forward(self, x, sublayer):\n",
    "      return x + self.drop(sublayer(self.norm(x)))\n",
    "\n",
    "# I simply let the model learn the positional embeddings in this notebook, since this\n",
    "# almost produces identital results as using sin/cosin functions embeddings, as claimed\n",
    "# in the original transformer paper. Note also that in the original paper, they multiplied\n",
    "# the token embeddings by a factor of sqrt(d_embed), which I do not do here.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''Encoder = token embedding + positional embedding -> a stack of N EncoderBlock -> layer norm'''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_embed = config.d_embed\n",
    "        self.tok_embed = nn.Embedding(config.encoder_vocab_size, config.d_embed)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed))\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(config) for _ in range(config.N_encoder)])\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.norm = nn.LayerNorm(config.d_embed)\n",
    "\n",
    "    def forward(self, input, mask=None):\n",
    "        x = self.tok_embed(input)\n",
    "        x_pos = self.pos_embed[:, :x.size(1), :]\n",
    "        x = self.dropout(x + x_pos)\n",
    "        for layer in self.encoder_blocks:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    '''EncoderBlock: self-attention -> position-wise fully connected feed-forward layer'''\n",
    "    def __init__(self, config):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.atten = MultiHeadedAttention(config.h, config.d_embed, config.dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(config.d_embed, config.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.d_ff, config.d_embed)\n",
    "        )\n",
    "        self.residual1 = ResidualConnection(config.d_embed, config.dropout)\n",
    "        self.residual2 = ResidualConnection(config.d_embed, config.dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # self-attention\n",
    "        x = self.residual1(x, lambda x: self.atten(x, x, x, mask=mask))\n",
    "        # position-wise fully connected feed-forward layer\n",
    "        return self.residual2(x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.linear = nn.Linear(config.d_embed, num_classes)\n",
    "\n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.encoder(x, pad_mask)\n",
    "        return  self.linear(torch.mean(x,-2))"
   ],
   "metadata": {
    "id": "gxqdJRLXyHin"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    encoder_vocab_size: int\n",
    "    d_embed: int\n",
    "    # d_ff is the dimension of the fully-connected  feed-forward layer\n",
    "    d_ff: int\n",
    "    # h is the number of attention head\n",
    "    h: int\n",
    "    N_encoder: int\n",
    "    max_seq_len: int\n",
    "    dropout: float\n",
    "\n",
    "def make_model(config):\n",
    "    model = Transformer(config, num_classes).to(DEVICE)\n",
    "    # initialize model parameters\n",
    "    # it seems that this initialization is very important!\n",
    "    for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    return model"
   ],
   "metadata": {
    "id": "gxYYnrC-FAgI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model"
   ],
   "metadata": {
    "id": "1HdgZPJBoKY2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader):\n",
    "    model.train()\n",
    "    losses, acc, count = [], 0, 0\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for idx, (x, y)  in  pbar:\n",
    "        optimizer.zero_grad()\n",
    "        features= x.to(DEVICE)\n",
    "        labels  = y.to(DEVICE)\n",
    "        pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))\n",
    "        pred = model(features, pad_mask)\n",
    "\n",
    "        loss = loss_fn(pred, labels).to(DEVICE)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        acc += (pred.argmax(1) == labels).sum().item()\n",
    "        count += len(labels)\n",
    "        # report progress\n",
    "        if idx>0 and idx%50 == 0:\n",
    "            pbar.set_description(f'train loss={loss.item():.4f}, train_acc={acc/count:.4f}')\n",
    "    return np.mean(losses), acc/count\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs):\n",
    "    for ep in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, test_loader)\n",
    "        print(f'ep {ep}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}')\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            features = x_test.to(DEVICE)\n",
    "            labels  = y_test.to(DEVICE)\n",
    "            pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))\n",
    "            pred = model(features, pad_mask)\n",
    "            loss = loss_fn(pred,labels).to(DEVICE)\n",
    "            losses.append(loss.item())\n",
    "            acc = (pred.argmax(1) == labels).sum().item()\n",
    "            count = len(labels)\n",
    "    return np.mean(losses), acc/count"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ydp6IfBrkYnL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "Transformer                                        --\n",
      "├─Encoder: 1-1                                     --\n",
      "│    └─Embedding: 2-1                              480,000\n",
      "│    └─ModuleList: 2-2                             --\n",
      "│    │    └─EncoderBlock: 3-1                      12,704\n",
      "│    └─Dropout: 2-3                                --\n",
      "│    └─LayerNorm: 2-4                              64\n",
      "├─Linear: 1-2                                      132\n",
      "===========================================================================\n",
      "Total params: 492,900\n",
      "Trainable params: 492,900\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(encoder_vocab_size = vocab_size,\n",
    "                     d_embed = 32,\n",
    "                     d_ff = 4*32,\n",
    "                     h = 1,\n",
    "                     N_encoder = 1,\n",
    "                     max_seq_len = max_seq_len,\n",
    "                     dropout = 0.1\n",
    "                     )\n",
    "model = make_model(config)\n",
    "print(torchinfo.summary(model))\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIau66WRlzRm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650855897178,
     "user_tz": 240,
     "elapsed": 2424,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "7c719890-df00-4140-ac70-c302f06ea837"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train loss=0.2858, train_acc=0.8969: 100%|██████████| 938/938 [00:08<00:00, 116.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ep 0: val_loss=0.2391, val_acc=0.9201\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train loss=0.2265, train_acc=0.9404: 100%|██████████| 938/938 [00:08<00:00, 110.53it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ep 1: val_loss=0.2436, val_acc=0.9200\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train loss=0.1170, train_acc=0.9524: 100%|██████████| 938/938 [00:08<00:00, 112.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ep 2: val_loss=0.2472, val_acc=0.9216\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, epochs=3)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qo7RYNx0lzRm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650855929754,
     "user_tz": 240,
     "elapsed": 32580,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "eabe9cb6-80b3-4330-bb01-a7d671b09b5e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## News classification example"
   ],
   "metadata": {
    "id": "XBSr0A9noh2i"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is a Sci/Tec news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def classify_news(news):\n",
    "    x_text = tokenizer(news.lower())[0:max_seq_len]\n",
    "    x_int = torch.tensor([[word_to_id.get(word, UNK) for word in x_text]]).to(DEVICE)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_int).argmax(1).item() + 1\n",
    "    print(f\"This is a {ag_news_label[pred]} news\")\n",
    "\n",
    "# The model correctly classifies a theoretical physics news as Sci/Tec news, :-)\n",
    "news = \"\"\"The conformal bootstrapDavid Poland1,2and David Simmons-Duﬃn2*The conformal bootstrap was\n",
    "proposed in the 1970s as a strategy for calculating the properties of second-order phasetransitions.\n",
    "After spectacular success elucidating two-dimensional systems, little progress was made on systems in\n",
    " higher dimensions until a recent renaissance beginning in 2008. We report on some of the main results and\n",
    "  ideas from thisrenaissance, focusing on new determinations of critical exponents and correlation\n",
    "  functions in the three-dimensional Ising and O(N) models.\n",
    "\"\"\"\n",
    "classify_news(news)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaAPcoPTkYnM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1650855929754,
     "user_tz": 240,
     "elapsed": 5,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "3dd6e44b-8f79-4d8e-ee9b-6ba14c4e47b4"
   }
  }
 ]
}