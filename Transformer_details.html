<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Transformer_details</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
  body {
    padding: 2cm; 
  }
}
</style>


</head>

<body>

<h1 id="toc_0">Some details of the transformer model</h1>

<p>This short note contains some details of the transformer model that I
found a little bit confusing when I first tried to implement it from
scratch. It is not intended to be a complete explanation of the
transformer model, since there is already a ton of useful material that
one can find online.</p>

<h2 id="toc_1">1. Notation</h2>

<ul>
<li>\(d_{B}:\) batch size</li>
<li>\(d_{S}:\) sequence length</li>
<li>\(d_{E}\) : embedding dimension</li>
<li>\(h\) : numbers of attention head</li>
<li>\(d_{k}=d_{E}/h:\) diemension of each attention head</li>
</ul>

<p>For the dimensions of a tensor  \(X\), I will denote it as follows
\[X:\left(d_{1},d_{2},....d_{n}\right),\] where \(d_{i}\) is the size of
the \(i\)-th dimension of \(X\).</p>

<h2 id="toc_2">2. Encoder</h2>

<p>In this section, I try to describe how the encoder part of the
transformer model works, especially the multi-head self-attention layer,
with some code for implementing it in Pytorch.</p>

<h3 id="toc_3">2.1 Padding and embedding</h3>

<p>Let&#39;s have in mind the machine translation problem, where the training
data will be sentence pairs in two different languages. We usually
separate the training set into batches with a batch size of \(d_{B}\). For
the input to the encoder part of the transformer model, each batch will
contains \(d_{B}\) sentences (assuming that they are tokenized). The lengths of
the sentences in each batch may be different. So we find the length of
the longest sentence, denote it as \(d_{S}\), and pad the sentences with
length smaller than \(d_{S}\) with zeros (or other padding tokens), such
that all the sentences in each batch have the same length \(d_{S}\). After
this, the input tensor \(X_{\text{input}}\) has dimension</p>

<p>\[X_{\text{input}}:\left(d_{B},d_{S}\right).\] For each word in the
sentences, we map it to a \(d_{E}\)-dimensional embedding vector, and we
also use a \(d_{E}\)-dimensional position embedding to encode the
positional information of the words in each sentence. After the
embedding layer, our embedded input \(X_{\text{input}}^{\text{E}}\) will
have dimension</p>

<p>\[X_{\text{input}}^{\text{E}}:\left(d_{B},d_{S},d_{E}\right).\]</p>

<h3 id="toc_4">2.2 Multi-head self-attention</h3>

<p>\(X_{\text{input}}^{\text{E}}\) will be the input to the multi-head
self-attention layer. The formula for doing attention is usually written
as follows
\[\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V.\]
Let&#39;s see in detail how is formula is implemented.</p>

<p>From \(X_{\text{input}}^{\text{E}}\), we first construct the query, key,
and value tensors, which we denote as \(Q,K\) and \(V\) respectively. They
are simply obtained by multiplying \(X_{\text{input}}\) with tensors
\(W_{\text{query}},\) \(W_{\text{key}},\)\(W_{\text{value}}\), which have
dimensions:
\[W_{\text{query}},W_{\text{key}},W_{\text{value}}:\left(d_{E},d_{E}=h\times d_{k}\right).\]
Here we are stacking the \(W\) matrices for different heads together along
the second dimension. So the query, key and values tensors \(Q,K,V\) have
dimensions 
\[\begin{aligned}
Q &amp; =X_{\text{input}}^{\text{E}}W_{\text{query}}:\left(d_{B},d_{S},d_{E}\right)\rightarrow\left(d_{B},h,d_{S},d_{k}\right),\\
K &amp; =X_{\text{input}}^{\text{E}}W_{\text{key}}\ \ :\left(d_{B},d_{S},d_{E}\right)\rightarrow\left(d_{B},h,d_{S},d_{k}\right),\\
V &amp; =X_{\text{input}}^{\text{E}}W_{\text{value}}:\left(d_{B},d_{S},d_{E}\right)\rightarrow\left(d_{B},h,d_{S},d_{k}\right).\end{aligned}\]
The dimensions on the left of the arrows are what we get after the matrix
multiplication, but we then reorganize the tensors such that they have
the dimensions shown after the arrows above. This is done in Pytorch as
follows</p>

<div><pre><code class="language-none">Q = torch.matmul(XEinput, Wq).view(d_B, -1, h, d_k).transpose(1,2)
K = torch.matmul(XEinput, Wk).view(d_B, -1, h, d_k).transpose(1,2)
V = torch.matmul(XEinput, Wv).view(d_B, -1, h, d_k).transpose(1,2)</code></pre></div>

<p>where I have tried to keep the variable names as close as possible to
the Latex notation. In actual implementation, one would register the
matrix multiplication above as a linear layer, since we will need to do
gradient descent on the matrix elements of the \(W\) matrices.</p>

<p>The next step is to do self-attention. This means that we take the dot
product of the query tensor \(Q\) and the key tensor \(K\). This is done as
follows. We transpose the last two dimensions of \(K,\) and use
torch.matmul, after which we get a tensor \(S\) of dimensions
\(\left(d_{B},h,d_{S},d_{S}\right)\): \[\begin{aligned}
S\equiv QK^{T} &amp; :\left(d_{B},h,d_{S},d_{k}\right)\times\left(d_{B},h,d_{k},d_{S}\right)=\left(d_{B},h,d_{S},d_{S}\right).\end{aligned}\]
In Pytorch, this is simply implemented by</p>

<div><pre><code class="language-none">S = torch.matmul(Q, K.transpose(-2,-1))</code></pre></div>

<p>Next, we compute the softmax of \(S\). Here, we want to mask out the
padding tokens in the sequences so that they will not participate in the
softmax computation. We simply assign a very large negative value to the
elements of \(S\) that correspond to the padding tokens. The is done as
follows:</p>

<div><pre><code class="language-none"># PAD is the integer corresponding to the padding token
mask = (Xinput == PAD).view(d_B, 1, 1, d_S) 
S = S.masked_fill(mask, float(&#39;-inf&#39;)) 
S = torch.nn.functional.softmax(S, -1)/math.sqrt(self.d_E)</code></pre></div>

<p>And the softmax above is along the last dimension of \(S\), after which
the dimensions of \(S\) do not change.</p>

<p>The final step of self-attention is to compute the average of the value
tensor \(V\) with weights given by \(S\). This is simply a matrix
multiplication:</p>

<p>\[\begin{aligned}
\text{softmax}\left(QK^{T}\right)V &amp; :\left(d_{B},h,d_{S},d_{S}\right)\times\left(d_{B},h,d_{S},d_{k}\right)=\left(d_{B},h,d_{S},d_{k}\right)\rightarrow\left(d_{B},d_{S},d_{E}\right)\end{aligned}\]
where after the matrix multiplication, we transpose the second and third
dimension, and flatten the last two dimension such that the output of an
attention layer has the same dimensions as its input. In Pytorch, this
is simply</p>

<p><code>x = torch.matmul(S, V).view(d_B, -1, h*d_k)</code></p>

<p>After the self-attension, one adds a fully connected layer (which does not change the dimension of \(x\) above). And that is basically it for the encoder of the transformer model.</p>

<p>To be continued...</p>



<script type="text/x-mathjax-config">
(function () {

MathJax.Hub.Config({
	'showProcessingMessages': false,
	'messageStyle': 'none'
});

if (typeof MathJaxListener !== 'undefined') {
	MathJax.Hub.Register.StartupHook('End', function () {
		MathJaxListener.invokeCallbackForKey_('End');
	});
}

})();
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</body>

</html>
