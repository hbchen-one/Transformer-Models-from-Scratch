{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Encoder_only_transformer_v3.ipynb",
   "provenance": [
    {
     "file_id": "1P7oU2EWQ1Qk17N9NutZv9FV1a_hMGUoR",
     "timestamp": 1647373127550
    }
   ],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, I train a encoder-only transformer to do text classification on the AG_NEWS dataset.\n",
    "Text classification seems to be a pretty simple task, and using transformer is probably overkill. But this is my first time implementing the transformer structure from scratch (including the self-attention layers), and it was fun :-)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# some commands in th   is notebook require torchtext 0.12.0\n",
    "!pip install --upgrade torchtext"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "Mtq3abS2lL_i",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647951914665,
     "user_tz": 240,
     "elapsed": 34635,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "bc9fe360-e5e5-4928-9f00-fcf9c90d58ed",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchdata\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ],
   "metadata": {
    "id": "bQEBuaIm5s9R",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647951933471,
     "user_tz": 240,
     "elapsed": 144,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "56973d95-12fe-41ac-b1e7-b4dfa61c7055"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing"
   ],
   "metadata": {
    "id": "SmM0VcYnCC8M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# One can easily modify the data processing part of this code to accommodate for   other datasets for text classification listed in https://pytorch.org/text/stable/datasets.html#text-classification\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "num_classes = len(set([label for (label, text) in train_iter]))\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ],
   "metadata": {
    "id": "7UdYsN6J5uke",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647951941724,
     "user_tz": 240,
     "elapsed": 3791,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# see an example of the dateset\n",
    "next(iter(train_iter))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HchVgEXWlqLz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647951941844,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "31db3c45-9b62-4801-d9bf-5ffc5d2a5899"
   },
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(3,\n \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# convert the labels to be in range(0, num_classes)\n",
    "y_train = torch.tensor([label-1 for (label, text) in train_iter])\n",
    "y_test  = torch.tensor([label-1 for (label, text) in test_iter])\n",
    "\n",
    "# There are many \"\\\\\" in the texts in the AG_news dataset, we get rid of them.\n",
    "train_iter = ((label, text.replace(\"\\\\\", \" \")) for label, text in train_iter)\n",
    "test_iter  = ((label, text.replace(\"\\\\\", \" \")) for label, text in test_iter)\n",
    "\n",
    "# tokenize the texts, and truncate the number of words in each text to max_seq_len\n",
    "max_seq_len = 100\n",
    "x_train_texts = [tokenizer(text.lower())[0:max_seq_len]\n",
    "                 for (label, text) in train_iter]\n",
    "x_test_texts  = [tokenizer(text.lower())[0:max_seq_len]\n",
    "                 for (label, text) in test_iter]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DeTWUptkYnG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647951956074,
     "user_tz": 240,
     "elapsed": 12321,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "9caf6766-acc3-4f05-d8c7-ce5713c44905"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# build the vocabulary and word-to-integer map\n",
    "counter = collections.Counter()\n",
    "for text in x_train_texts:\n",
    "    counter.update(text)\n",
    "\n",
    "vocab_size = 15000\n",
    "most_common_words = np.array(counter.most_common(vocab_size - 2))\n",
    "vocab = most_common_words[:,0]\n",
    "\n",
    "# indexes for the padding token, and unknown tokens\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "word_to_id = {vocab[i]: i + 2 for i in range(len(vocab))}"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MYVE8HSGkYnH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647951957290,
     "user_tz": 240,
     "elapsed": 1225,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# map the words in the training and test texts to integers\n",
    "x_train = [torch.tensor([word_to_id.get(word, UNK) for word in text])\n",
    "           for text in x_train_texts]\n",
    "x_test  = [torch.tensor([word_to_id.get(word, UNK) for word in text])\n",
    "          for text in x_test_texts]\n",
    "x_test = torch.nn.utils.rnn.pad_sequence(x_test,\n",
    "                                batch_first=True, padding_value = PAD)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "I7-4KQI8kYnH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647951961053,
     "user_tz": 240,
     "elapsed": 3765,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# constructing the dataset in order to be compatible with torch.utils.data.Dataloader\n",
    "class AGNewsDataset:\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.features[item], self.labels[item]\n",
    "\n",
    "\n",
    "train_dataset = AGNewsDataset(x_train, y_train)\n",
    "test_dataset  = AGNewsDataset(x_test, y_test)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hJe8LAUNkYnI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647951961053,
     "user_tz": 240,
     "elapsed": 2,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# collate_fn to be used in torch.utils.data.DataLoader().\n",
    "# It pads the texts in each batch such that they have the same sequence length.\n",
    "def pad_sequence(batch):\n",
    "    texts  = [text for text, label in batch]\n",
    "    labels = torch.tensor([label for text, label in batch])\n",
    "    texts_padded = torch.nn.utils.rnn.pad_sequence(texts,\n",
    "                                batch_first=True, padding_value = PAD)\n",
    "    return texts_padded, labels\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "                        collate_fn = pad_sequence)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True,\n",
    "                        collate_fn = pad_sequence)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ov3tX4sRkYnI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647951961054,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building the encoder-only transformer model for text classification"
   ],
   "metadata": {
    "id": "zNVoCKz0CM3g"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# One can also replace the MultiHeadedAttention class here with\n",
    "# torch.nn.MultiheadAttention provided by pytorch.\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0 # check the h number\n",
    "        self.d_k = d_model//h\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        # 4 linear layers: WQ WK WV and final linear mapping WO\n",
    "        self.WQ = nn.Linear(d_model, d_model)\n",
    "        self.WK = nn.Linear(d_model, d_model)\n",
    "        self.WV = nn.Linear(d_model, d_model)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x_query, x_key, x_value, mask=None):\n",
    "        nbatches = x_query.size(0) # get batch size\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        # parttion into h sections，switch 2,3 axis for computation.\n",
    "        #LHS query, key, value dimensions: nbatch*h*dseq*dk\n",
    "        #x dimension nbatch*dseq*d_model\n",
    "        query = self.WQ(x_query).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "        key   = self.WK(x_key).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "        value = self.WV(x_value).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        # query, key, value all have size: nbatch*h*d_seq*d_k\n",
    "        # scores has size: nbatch*h*d_seq*d_seq\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.d_model)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        p_attn = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        x = torch.matmul(p_attn, value)\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        # x dimensions:nbtach*dseq*(h*dk)\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.linear(x) # final linear layer\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "  '''residual connection: x + dropout(sublayer(layernorm(x))) '''\n",
    "  def __init__(self, dim, dropout):\n",
    "      super().__init__()\n",
    "      self.drop = nn.Dropout(dropout)\n",
    "      self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "  def forward(self, x, sublayer):\n",
    "      return x + self.drop(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "# Encoder-only Transformer = words embedding + position embedding -> N stack of EncoderBlock ->full connected layer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, max_len, vocab_size, h, d_model, dropout, N):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model) #words embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model) #position embedding\n",
    "        self.encoder_layer = nn.Sequential(\n",
    "            *[EncoderBlock(h, d_model, 4*d_model) for _ in range(N)]\n",
    "        )\n",
    "        self.linear = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, input, mask=None):\n",
    "        x = self.embed(input) * math.sqrt(self.d_model)\n",
    "        x_pos = self.pos_embed(torch.tensor(range(input.size(-1))).to(DEVICE))\n",
    "        x = x + x_pos\n",
    "        for layer in self.encoder_layer:\n",
    "            x = layer(x, mask)\n",
    "        return self.linear(torch.mean(x,-2))\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, h, d_model, d_ff, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attn = MultiHeadedAttention(h, d_model, dropout)\n",
    "        #self.attn = nn.MultiheadAttention(d_model, h,  batch_first=True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.residual1 = ResidualConnection(d_model, dropout)\n",
    "        self.residual2 = ResidualConnection(d_model, dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.residual1(x, lambda x: self.attn(x, x, x, mask =mask))\n",
    "        # positionwise feed-forwad\n",
    "        x = self.residual2(x, lambda x: self.feed_forward(x))\n",
    "        return x\n"
   ],
   "metadata": {
    "id": "gxqdJRLXyHin",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647952036652,
     "user_tz": 240,
     "elapsed": 168,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# d_model is the embedding dimension\n",
    "d_model = 32\n",
    "# h is the number of attention head\n",
    "# N is the number of encoder blocks\n",
    "# For the text classification problem in this notebook, h=1 and N=1 are already enough.\n",
    "h = 1\n",
    "N = 1\n",
    "\n",
    "dropout = 0.1\n",
    "model =Transformer(max_seq_len, vocab_size, h, d_model, dropout, N).to(DEVICE)\n",
    "# initialize model parameters\n",
    "# it seems that this initialization is very important!\n",
    "for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)"
   ],
   "metadata": {
    "id": "gxYYnrC-FAgI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647952257128,
     "user_tz": 240,
     "elapsed": 2618,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    }
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss, acc, count = 0,0,0\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for idx, (x, y)  in  pbar:\n",
    "        optimizer.zero_grad()\n",
    "        features= x.to(DEVICE)\n",
    "        labels  = y.to(DEVICE)\n",
    "        pred = model(features, (features==0).unsqueeze(-2).unsqueeze(1).to(DEVICE))\n",
    "\n",
    "        loss = loss_fn(pred, labels).to(DEVICE)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        acc += (pred.argmax(1) == labels).sum().item()\n",
    "        count += len(labels)\n",
    "        # report progress\n",
    "        if idx%50 == 0:\n",
    "            val_acc, val_loss = evaluate(x_test, y_test)\n",
    "            pbar.set_description(f\"Train acc={acc/count:.3f}, Train loss={total_loss.item()/(idx+1):.3f}, test acc = {val_acc:.3f}, test loss= {val_loss:.5f}\")\n",
    "\n",
    "def train(model,dataloader, epochs):\n",
    "    for ep in range(epochs):\n",
    "        train_epoch(model,dataloader)\n",
    "\n",
    "def evaluate(x_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features= x_test.to(DEVICE)\n",
    "        labels  = y_test.to(DEVICE)\n",
    "        pred = model(features, (features==0).unsqueeze(-2).unsqueeze(1).to(DEVICE))\n",
    "        loss = loss_fn(pred,labels).to(DEVICE)\n",
    "        acc = (pred.argmax(1) == labels).sum().item()\n",
    "        count = len(labels)\n",
    "    return acc/count, loss.item()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ydp6IfBrkYnL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647952269216,
     "user_tz": 240,
     "elapsed": 153,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train acc=0.903, Train loss=0.284, test acc = 0.921, test loss= 0.23204: 100%|██████████| 938/938 [01:13<00:00, 12.83it/s]\n",
      "Train acc=0.944, Train loss=0.159, test acc = 0.920, test loss= 0.24258: 100%|██████████| 938/938 [01:11<00:00, 13.13it/s]\n",
      "Train acc=0.959, Train loss=0.112, test acc = 0.915, test loss= 0.25909: 100%|██████████| 938/938 [01:13<00:00, 12.74it/s]\n",
      "Train acc=0.969, Train loss=0.082, test acc = 0.917, test loss= 0.31659: 100%|██████████| 938/938 [01:13<00:00, 12.79it/s]\n",
      "Train acc=0.977, Train loss=0.061, test acc = 0.912, test loss= 0.38330: 100%|██████████| 938/938 [01:11<00:00, 13.16it/s]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "hist = train(model, train_loader, epochs=5)\n",
    "# strangely, the test accuracy is higher than the training accuracy during the first epoch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sci/Tec news\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "# The model correctly classifies a theoretical physics news as Sci/Tec news, :-)\n",
    "ex_text = \"\"\"The conformal bootstrapDavid Poland1,2and David Simmons-Duﬃn2*The conformal bootstrap was\n",
    "proposed in the 1970s as a strategy for calculating the properties of second-order phasetransitions.\n",
    "After spectacular success elucidating two-dimensional systems, little progress was made on systems in\n",
    " higher dimensions until a recent renaissance beginning in 2008. We report on some of the main results and\n",
    "  ideas from thisrenaissance, focusing on new determinations of critical exponents and correlation\n",
    "  functions in the three-dimensional Ising and O(N) models.\n",
    "\"\"\"\n",
    "\n",
    "x_ex_text = tokenizer(ex_text.lower())[0:max_seq_len]\n",
    "x_ex_int = torch.tensor([[word_to_id.get(word, UNK) for word in x_ex_text]]).to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(x_ex_int).argmax(1).item() + 1\n",
    "\n",
    "print(f\"This is a {ag_news_label[pred]} news\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaAPcoPTkYnM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1647952357616,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "Hongbin Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14161707569992931612"
     }
    },
    "outputId": "416c6841-5c75-4bf0-9bfa-8717fa19ec9f"
   }
  }
 ]
}