{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Encoder_only_transformer_ArXiv_classification.ipynb","provenance":[{"file_id":"1P7oU2EWQ1Qk17N9NutZv9FV1a_hMGUoR","timestamp":1647373127550}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In this notebook, I train a encoder-only transformer to classify arXiv titles and abstracts into their main category. This is a simple task, and the purpose of this notebook is to make myself more familiar with the Transformer structure by implemeting it from scratch. "],"metadata":{"id":"e2XDat3Kz9iM"}},{"cell_type":"code","source":["# import packages\n","import torch\n","import json\n","import torchtext\n","import collections\n","import numpy as np\n","import torch.nn as nn\n","import math\n","import copy\n","import torch.nn.functional as functional\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from torch.utils.data import random_split\n","from collections import Counter\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)"],"metadata":{"id":"bQEBuaIm5s9R","executionInfo":{"status":"ok","timestamp":1648928364905,"user_tz":240,"elapsed":7543,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6b2857d-befb-4734-c4af-1b5db90f4a1f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"markdown","source":["# Data processing"],"metadata":{"id":"SmM0VcYnCC8M"}},{"cell_type":"code","source":["arxiv_url = \"https://www.kaggle.com/datasets/Cornell-University/arxiv/download\""],"metadata":{"id":"15xaCPve1tfr","executionInfo":{"status":"ok","timestamp":1648929382663,"user_tz":240,"elapsed":134,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2l6pbH5b2Xur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import urllib.request\n","with urllib.request.urlopen(arxiv_url) as f:\n","    html = f.read().decode('utf-8')"],"metadata":{"id":"bkhqBsT21-Ts","executionInfo":{"status":"ok","timestamp":1648929388967,"user_tz":240,"elapsed":576,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# The arXiv dataset is pretty large (3.3) is can be downloaded from Kaggle:\n","# https://www.kaggle.com/datasets/Cornell-University/arxiv\n","\n","data_file = '/Users/hongbinchen/Downloads/arxiv-metadata-oai-snapshot.json'\n","\n","\"\"\" Using `yield` to load the JSON file in a loop to prevent Python memory issues if JSON is loaded directly\"\"\"\n","\n","def get_metadata():\n","    with open(data_file, 'r') as f:\n","        for line in f:\n","            yield line"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"s6FV1zWSyYZb"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["metadata = get_metadata()\n","categories = []\n","for paper in metadata:\n","    cates = json.loads(paper)['categories']\n","    categories.append(cates.split(\".\")[0])"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"jahtPzbuyYZc"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["metadata = get_metadata()\n","counter = Counter()\n","for paper in metadata:\n","    cates = json.loads(paper)['categories']\n","    counter.update([cates.split()[0]])"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"TSOa5a8cyYZc"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["num_classes = 5\n","classes = counter.most_common(num_classes)\n","classes = {classes[i][0]:i for i in range(len(classes))}"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"tvnwPv5dyYZc"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["import torchtext\n","tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"bAD11HGmyYZd"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["metadata = get_metadata()\n","max_len = 100\n","dataset = []\n","max_num_each_class = 10000\n","num_each_class = {cate: 0 for cate in classes}\n","for paper in metadata:\n","    paper_data = json.loads(paper)\n","    cate = paper_data['categories'].split()[0]\n","    if cate in classes and num_each_class[cate]<max_num_each_class:\n","        temp = paper_data['title']+\" \" + paper_data['abstract']\n","        temp.replace('\\n', ' ')\n","        temp = tokenizer(temp)[0:max_len]\n","        dataset.append((temp, classes[cate]))\n","        num_each_class[cate] += 1"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"ejRV-BxEyYZe"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["counter = Counter()\n","for x in dataset:\n","    counter.update(x[0])\n","vocab_size = 10000\n","vocab = counter.most_common(vocab_size-2)\n","UNK = 1\n","PAD = 0\n","word_to_int = {vocab[i][0]:i+2 for i in range(len(vocab))}\n","def word_to_int_fn(text):\n","    return torch.tensor([word_to_int.get(word, UNK) for word in text])\n","dataset = [(word_to_int_fn(text), label) for text, label in dataset]"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"0WlECsGDyYZf"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# split the dataset into 80% training, and 20% test\n","dataset_size = len(dataset)\n","train_size = dataset_size*8//10\n","train_data, test_data = random_split(dataset,\n","                                     [dataset_size-train_size, train_size],generator=torch.Generator().manual_seed(42)\n","                                     )"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"G3U3RthFyYZf"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["metadata = get_metadata()\n","categories = []\n","for paper in metadata:\n","    authors = json.loads(paper)['authors']\n","    if 'Hongbin Chen' in authors:\n","        print(json.loads(paper))"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"hnv_MybDyYZg"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# map the words in the training and test texts to integers\n","x_train = [x[0] for x in train_data]\n","y_train = torch.tensor([x[1] for x in train_data])\n","x_test = [x[0] for x in test_data]\n","y_test = torch.tensor([x[1] for x in test_data])\n","#x_test  = [torch.tensor([word_to_ID.get(word, UNK) for word in text])\n","#          for text in x_test_texts]\n","#x_test = torch.nn.utils.rnn.pad_sequence(x_test,\n","#                                batch_first=True, padding_value = PAD)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"I7-4KQI8kYnH"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["print(len(y_test))\n","print(len(y_train))"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"43DYDySCyYZg"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["class ArXivDataset:\n","    def __init__(self, features, labels):\n","        self.features = features\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, item):\n","        return self.features[item], self.labels[item]\n","\n","\n","train_dataset = ArXivDataset(x_train, y_train)\n","test_dataset = ArXivDataset(x_test, y_test)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"hJe8LAUNkYnI"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["def pad_sequence(batch):\n","    texts  = [text for text, label in batch]\n","    labels = torch.tensor([label for text, label in batch])\n","    texts_padded = torch.nn.utils.rnn.pad_sequence(texts,\n","                                batch_first=True, padding_value = PAD)\n","    return texts_padded, labels\n","\n","# each batch returned by dataloader will be padded such that all the texts in\n","# that batch have the same length as the longest text in that batch\n","train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n","                        collate_fn = pad_sequence)\n","test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True,\n","                        collate_fn = pad_sequence)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"ov3tX4sRkYnI"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["next(iter(test_dataloader))"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"xfXJyqYsyYZh"}},{"cell_type":"markdown","source":["# Building the encoder-only transformer model for text classification"],"metadata":{"id":"zNVoCKz0CM3g"}},{"cell_type":"code","source":["# one can also replace the MultiHeadedAttention class here with \n","# torch.nn.MultiheadAttention provided by pytorch\n","class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        \"Take in model size and number of heads.\"\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0 # check the h number\n","        self.d_k = d_model//h\n","        self.d_model = d_model\n","        self.h = h\n","        # 4 linear layers: WQ WK WV and final linear mapping WO \n","        self.WQ = nn.Linear(d_model, d_model)\n","        self.WK = nn.Linear(d_model, d_model)\n","        self.WV = nn.Linear(d_model, d_model)\n","        self.linear = nn.Linear(d_model, d_model)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x_query, x_key, x_value, mask=None):\n","        nbatches = x_query.size(0) # get batch size\n","        # 1) Do all the linear projections in batch from d_model => h x d_k \n","        # parttion into h sectionsï¼Œswitch 2,3 axis for computation. \n","        #LHS query, key, value dimensions: nbatch*h*dseq*dk\n","        #x dimension nbatch*dseq*d_model\n","        query = self.WQ(x_query).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n","        key   = self.WK(x_key).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n","        value = self.WV(x_value).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n","        # 2) Apply attention on all the projected vectors in batch.\n","        # query, key, value all have size: nbatch*h*d_seq*d_k\n","        # scores has size: nbatch*h*d_seq*d_seq\n","        scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.d_model)\n","        if mask is not None:\n","            scores = scores.masked_fill(mask, -1e9)\n","        p_attn = functional.softmax(scores, dim=-1)\n","        x = torch.matmul(p_attn, value)\n","        # 3) \"Concat\" using a view and apply a final linear. \n","        # x dimensions:nbtach*dseq*(h*dk)\n","        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n","        return self.linear(x) # final linear layer"],"metadata":{"id":"gxqdJRLXyHin"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderBlock(nn.Module):\n","    def __init__(self, h, d_model, d_ff, dropout):\n","        super(EncoderBlock, self).__init__()\n","        self.self_attn = MultiHeadedAttention(h, d_model, dropout)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(d_model, d_ff),\n","            nn.ReLU(),\n","            nn.Linear(d_ff, d_model),\n","            nn.Dropout(dropout)\n","        )\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        x = self.norm1(x + self.self_attn(x, x, x, mask))\n","        x = self.dropout(x)\n","        # positionwise feed-forwad \n","        return self.norm2(x + self.feed_forward(x))"],"metadata":{"id":"5bmTgc6KyVyk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Transformer = words embedding + position embedding -> N stack of EncoderBlock ->full connected layer \n","class Transformer(nn.Module):\n","    def __init__(self, encoder_layer, max_len, vocab_size, d_model, dropout, N):\n","        super(Transformer, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, d_model) #words embedding\n","        self.pos_embed = nn.Embedding(max_len, d_model) #position embedding\n","        self.encoder_layer = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(N)])\n","        self.linear = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, input, mask=None):\n","        x = self.embed(input)\n","        x_pos = self.pos_embed(torch.tensor(range(input.size(-1))).to(DEVICE))\n","        x = x + x_pos\n","        for layer in self.encoder_layer:\n","            x = layer(x, mask)\n","        return self.linear(torch.mean(x,-2))"],"metadata":{"id":"Tc7zwQF_zYYN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# paramters for the model structure\n","# d_model is the embedding dimension\n","d_model = 32\n","# d_ff is the dimension of the fully-connected layer\n","d_ff = 32\n","# h is the number of attention head\n","h = 2\n","dropout = 0.1\n","max_len = max_len\n","# N is the number of encoder blocks, for the text classification problem in this notebook,\n","# N = 1 is already enough \n","N = 1\n","\n","model =Transformer(EncoderBlock(h, d_model, d_ff, dropout),\n","                       max_len, vocab_size, d_model, dropout, N).to(DEVICE)\n","# initialize model parameters\n","# it seems that this initialization is very important!\n","for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)"],"metadata":{"id":"gxYYnrC-FAgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["(torch.tensor([[1,2,4],[2,4,5]])==0).unsqueeze(-2).unsqueeze(1)==0"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"FHu30sOyyYZj"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["def train_epoch(model, dataloader):\n","    model.train()\n","    total_loss, acc, count = 0,0,0\n","    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for idx, (x, y)  in  pbar:\n","        optimizer.zero_grad()\n","        features= x.to(DEVICE)\n","        labels  = y.to(DEVICE)\n","        pred = model(features, (features==0).unsqueeze(-2).unsqueeze(1).to(DEVICE))\n","        loss = loss_fn(pred, labels).to(DEVICE)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss\n","        acc += (pred.argmax(1) == labels).sum().item()\n","        count += len(labels)\n","        # report progress\n","        if idx%50 == 0:\n","            val_acc, val_loss = evaluate(test_dataloader)\n","            pbar.set_description(f\"Train acc={acc/count:.3f}, Train loss={total_loss.item()/(idx+1):.3f}, test acc = {val_acc:.3f}, test loss= {val_loss:.5f}\")\n","\n","def train(model,dataloader, epochs):\n","    for ep in range(epochs):\n","        train_epoch(model,dataloader)\n","\n","def evaluate(dataloder):\n","    model.eval()\n","    total_loss = 0\n","    total_acc = 0\n","    count = 0\n","    with torch.no_grad():\n","        for i, (x, y) in enumerate(dataloder):\n","            features= x.to(DEVICE)\n","            labels  = y.to(DEVICE)\n","            pred = model(features, (features==0).unsqueeze(-2).unsqueeze(1).to(DEVICE))\n","            total_loss += loss_fn(pred,labels).to(DEVICE)\n","            total_acc += (pred.argmax(1) == labels).sum().item()\n","            count += len(labels)\n","    return total_acc/count, total_loss/(i+1)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"Ydp6IfBrkYnL"}},{"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters())\n","loss_fn = nn.CrossEntropyLoss()\n","hist = train(model, train_dataloader, epochs=5)\n","# strangely, the test accuracy is higher than the training accuracy during the first epoch"],"metadata":{"id":"Y0thGAa_0K_m","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["int_to_classes = {item[1]:item[0] for item in classes.items()}"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"P6_JhtiWyYZk"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["int_to_classes"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"3vvhyv_7yYZk"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["ex_text = \"\"\"2D Ising Field Theory in a Magnetic Field: The Yang-Lee Singularity\n"," We study Ising Field Theory (the scaling limit of Ising model near the Curie critical point) in pure imaginary external magnetic field. We put particular emphasis on the detailed structure of the Yang-Lee edge singularity. While the leading singular behavior is controlled by the Yang-Lee fixed point (= minimal CFT îˆ¹2/5), the fine structure of the subleading singular terms is determined by the effective action which involves a tower of irrelevant operators. We use numerical data obtained through the \"Truncated Free Fermion Space Approach\" to estimate the couplings associated with two least irrelevant operators. One is the operator TTÂ¯, and we use the universal properties of the TTÂ¯ deformation to fix the contributions of higher orders in the corresponding coupling parameter Î±. Another irrelevant operator we deal with is the descendant Lâˆ’4LÂ¯âˆ’4Ï• of the relevant primary in îˆ¹2/5. The significance of this operator is that it is the lowest dimension operator which breaks integrability of the effective theory. We also establish analytic properties of the particle mass M (= inverse correlation length) as the function of complex magnetic field.\n","\"\"\"\n","\n","x_ex_text = tokenizer(ex_text.lower())[0:max_len]\n","x_ex_int = torch.tensor([[word_to_int.get(word, UNK) for word in x_ex_text]]).to(DEVICE)\n","\n","model.eval()\n","with torch.no_grad():\n","    pred = model(x_ex_int).argmax(1).item()\n","\n","print(f\"This is a {int_to_classes[pred]} paper\")"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"uaAPcoPTkYnM"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"IF5Ag9s_yYZk"}}]}