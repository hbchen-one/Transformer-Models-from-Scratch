{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer_Multi30k_German_to_English.ipynb","provenance":[{"file_id":"1bU3hHSQGY-MfTgfZaQWQzivJD__STGOR","timestamp":1650031322821},{"file_id":"1YhqP5st0yiBt4FJofOwY7g6GrzGZWQNW","timestamp":1649105742866},{"file_id":"1P7oU2EWQ1Qk17N9NutZv9FV1a_hMGUoR","timestamp":1647373127550}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["In this notebook, I train a transformer model for translating German to English. The model structure is the same as the original [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) paper. "],"metadata":{"id":"D23TRFotJ6bh"}},{"cell_type":"code","source":["!pip install sentencepiece --quiet\n","!pip install sacrebleu --quiet"],"metadata":{"id":"jtTpObu_cdP2","executionInfo":{"status":"ok","timestamp":1650208880961,"user_tz":240,"elapsed":6343,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["\n","import math\n","from dataclasses import dataclass\n","\n","import numpy as np\n","import sacrebleu\n","import sentencepiece as spm\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torchtext.datasets import Multi30k\n","from tqdm import tqdm\n","\n","torch.manual_seed(0)\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)"],"metadata":{"id":"ToFDXgFP5fys","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650208880962,"user_tz":240,"elapsed":7,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"7ab588c3-abaf-45da-9f61-269cc4d64f25"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["SRC = \"de\"\n","TRG = \"en\""],"metadata":{"id":"tlCncOUK4lUo","executionInfo":{"status":"ok","timestamp":1650208880962,"user_tz":240,"elapsed":5,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["# Get German and English tokenizers from SentencePiece"],"metadata":{"id":"Jq10RHNKZTDo"}},{"cell_type":"code","source":["train_iter = Multi30k(split='train', language_pair=(SRC, TRG))\n","f_de = open(\"Multi30k_de_text.txt\", \"w\")\n","f_en = open(\"Multi30k_en_text.txt\", \"w\")\n","for pair in train_iter:\n","    f_de.write(pair[0])\n","    f_en.write(pair[1])\n","f_de.close()\n","f_en.close()"],"metadata":{"id":"hz9u_PmSdLdc","executionInfo":{"status":"ok","timestamp":1650208880962,"user_tz":240,"elapsed":4,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["en_vocab_size = 8200\n","de_vocab_size = 10000\n","vocab_sizes = {\"en\": en_vocab_size, \"de\": de_vocab_size}"],"metadata":{"id":"N1eDUeBkFnea","executionInfo":{"status":"ok","timestamp":1650208880962,"user_tz":240,"elapsed":4,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# train sentencepiece models to get tokenizers\n","spm.SentencePieceTrainer.train\\\n","(f'--input=Multi30k_de_text.txt --model_prefix=Multi30k_de --user_defined_symbols=<pad> --vocab_size={de_vocab_size}')\n","spm.SentencePieceTrainer.train\\\n","(f'--input=Multi30k_en_text.txt --model_prefix=Multi30k_en --user_defined_symbols=<pad> --vocab_size={en_vocab_size}')\n","\n","# make segmenter instances and load the model files\n","de_sp = spm.SentencePieceProcessor()\n","de_sp.load('Multi30k_de.model')\n","en_sp = spm.SentencePieceProcessor()\n","en_sp.load('Multi30k_en.model')\n","\n","tokenizers = {\"en\": en_sp.encode_as_ids, \"de\": de_sp.encode_as_ids}\n","detokenizers = {\"en\":en_sp.decode_ids, \"de\":de_sp.decode_ids}\n","\n","# encode: text => id\n","print(en_sp.encode_as_pieces('This is a test'))\n","print(en_sp.encode_as_ids('This is a test'))\n","\n","# decode: id => text\n","print(en_sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n","print(en_sp.decode_ids([302, 258, 10, 4, 2395]))"],"metadata":{"id":"asaHCE3IZhrl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650208883383,"user_tz":240,"elapsed":909,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"42b665e1-2b45-4a59-ff53-099efa4b1ef0"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["['▁Th', 'is', '▁is', '▁a', '▁test']\n","[302, 258, 10, 4, 2395]\n","▁This is a test\n","This is a test\n"]}]},{"cell_type":"code","execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["['<unk>', '<s>', '</s>', '<pad>', '▁a', '.', '▁A', '▁in', '▁the', '▁on', '▁is', '▁man', '▁and', '▁of', '▁with', 's', 'ing', '▁', ',', '▁woman']\n","['<unk>', '<s>', '</s>', '<pad>', '.', '▁eine', '▁Ein', 'm', '▁in', '▁mit', ',', '▁und', '▁auf', '▁ein', '▁Mann', '▁einer', '▁Eine', 'n', '▁der', '▁Frau']\n"]}],"source":["print([en_sp.id_to_piece(id) for id in range(20)])\n","print([de_sp.id_to_piece(id) for id in range(20)])"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"zv9Q9YyAUqoh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650208883383,"user_tz":240,"elapsed":4,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"66f0b6bf-bf29-4a3a-d0bc-de4d9248b738"}},{"cell_type":"code","source":["# indeces of special symbols \n","UNK, BOS, EOS, PAD = 0, 1, 2, 3"],"metadata":{"id":"8grAiF0GfmYx","executionInfo":{"status":"ok","timestamp":1650208883893,"user_tz":240,"elapsed":512,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["# Data processing"],"metadata":{"collapsed":false,"id":"OuOHh7fF5Tn8"}},{"cell_type":"code","source":["train_iter = Multi30k(split='train', language_pair=(SRC, TRG))\n","valid_iter = Multi30k(split='valid', language_pair=(SRC, TRG))\n","test_iter  = Multi30k(split='test',  language_pair=(SRC, TRG))\n","\n","train_set = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in train_iter]\n","valid_set = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in valid_iter]\n","test_set  = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in test_iter]\n","print(len(train_set), len(valid_set), len(test_set))\n","for i in range(10):\n","   print(train_set[i])"],"metadata":{"id":"jh3Nxm7VaHIQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650208883896,"user_tz":240,"elapsed":13,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"0e6316a6-04f2-447d-bd86-975dfaabfeb1"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["29000 1014 1000\n","('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.', 'Two young, White males are outside near many bushes.')\n","('Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.', 'Several men in hard hats are operating a giant pulley system.')\n","('Ein kleines Mädchen klettert in ein Spielhaus aus Holz.', 'A little girl climbing into a wooden playhouse.')\n","('Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.', 'A man in a blue shirt is standing on a ladder cleaning a window.')\n","('Zwei Männer stehen am Herd und bereiten Essen zu.', 'Two men are at the stove preparing food.')\n","('Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.', 'A man in green holds a guitar while the other man observes his shirt.')\n","('Ein Mann lächelt einen ausgestopften Löwen an.', 'A man is smiling at a stuffed lion')\n","('Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.', 'A trendy girl talking on her cellphone while gliding slowly down the street.')\n","('Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.', 'A woman with a large purse is walking by a gate.')\n","('Jungen tanzen mitten in der Nacht auf Pfosten.', 'Boys dancing on poles in the middle of the night.')\n"]}]},{"cell_type":"code","execution_count":45,"outputs":[],"source":["max_seq_len = 50\n","def tokenize_dataset(dataset):\n","    'tokenize a dataset and add [BOS] and [EOS] to the beginning and end of the sentences'\n","    return [(torch.tensor([BOS]+tokenizers[SRC](src_text)[0:max_seq_len-2]+[EOS]),\n","             torch.tensor([BOS]+tokenizers[TRG](trg_text)[0:max_seq_len-2]+[EOS]))\n","            for src_text, trg_text in dataset]\n","          \n","train_tokenized = tokenize_dataset(train_set)\n","valid_tokenized = tokenize_dataset(valid_set)\n","test_tokenized  = tokenize_dataset(test_set)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"gF5sy0p65ToA","executionInfo":{"status":"ok","timestamp":1650208885984,"user_tz":240,"elapsed":2099,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}}},{"cell_type":"code","source":["class TranslationDataset(Dataset):\n","    'create a dataset for torch.utils.data.DataLoader() '\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","\n","def pad_sequence(batch):\n","    'collate function for padding setences such that all \\\n","    the sentences in the batch have the same lence'\n","    src_seqs  = [src for src, trg in batch]\n","    trg_seqs  = [trg for src, trg in batch]\n","    src_padded = torch.nn.utils.rnn.pad_sequence(src_seqs,\n","                                batch_first=True, padding_value = PAD)\n","    trg_padded = torch.nn.utils.rnn.pad_sequence(trg_seqs,\n","                                batch_first=True, padding_value = PAD)\n","    return src_padded, trg_padded\n"],"metadata":{"id":"8RSKKEGTHICe","executionInfo":{"status":"ok","timestamp":1650208885984,"user_tz":240,"elapsed":3,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["batch_size = 128\n","\n","class Dataloaders:\n","    'Dataloaders contains train_loader, test_loader and valid_loader for trainning and evaluation '\n","    def __init__(self):\n","        self.train_dataset = TranslationDataset(train_tokenized)\n","        self.valid_dataset = TranslationDataset(valid_tokenized)\n","        self.test_dataset  = TranslationDataset(test_tokenized)\n","        \n","        # each batch returned by dataloader will be padded such that all the texts in\n","        # that batch have the same length as the longest text in that batch\n","        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size,\n","                                                shuffle=True, collate_fn = pad_sequence)\n","        \n","        self.test_loader = torch.utils.data.DataLoader(self.test_dataset, batch_size=batch_size,\n","                                                shuffle=True, collate_fn=pad_sequence)\n","        \n","        self.valid_loader = torch.utils.data.DataLoader(self.valid_dataset, batch_size=batch_size,\n","                                                shuffle=True, collate_fn=pad_sequence)\n"],"metadata":{"id":"WgeG2xQwFmIZ","executionInfo":{"status":"ok","timestamp":1650208885984,"user_tz":240,"elapsed":2,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["# Transformer Model"],"metadata":{"id":"6t5Bkt_LOu0p"}},{"cell_type":"code","execution_count":48,"outputs":[],"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_embed, dropout=0.0):\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_embed % h == 0 # check the h number\n","        self.d_k = d_embed//h\n","        self.d_embed = d_embed\n","        self.h = h\n","        self.WQ = nn.Linear(d_embed, d_embed)\n","        self.WK = nn.Linear(d_embed, d_embed)\n","        self.WV = nn.Linear(d_embed, d_embed)\n","        self.linear = nn.Linear(d_embed, d_embed)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x_query, x_key, x_value, mask=None):\n","        nbatch = x_query.size(0) # get batch size\n","        # 1) Linear projections to get the multi-head query, key and value tensors\n","        # x_query, x_key, x_value dimension: nbatch * seq_len * d_embed\n","        # LHS query, key, value dimensions: nbatch * h * seq_len * d_k\n","        query = self.WQ(x_query).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n","        key   = self.WK(x_key).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n","        value = self.WV(x_value).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n","        # 2) Attention\n","        # scores has dimensions: nbatch * h * seq_len * seq_len\n","        scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.d_embed)\n","        # 3) Mask out padding tokens and future tokens\n","        if mask is not None:\n","            scores = scores.masked_fill(mask, float('-inf'))\n","        # p_atten dimensions: nbatch * h * seq_len * seq_len\n","        p_atten = torch.nn.functional.softmax(scores, dim=-1)\n","        p_atten = self.dropout(p_atten)\n","        # x dimensions: nbatch * h * seq_len * d_k\n","        x = torch.matmul(p_atten, value)\n","        # x now has dimensions:nbtach * seq_len * d_embed\n","        x = x.transpose(1, 2).contiguous().view(nbatch, -1, self.d_embed)\n","        return self.linear(x) # final linear layer\n","\n","\n","class ResidualConnection(nn.Module):\n","  '''residual connection: x + dropout(sublayer(layernorm(x))) '''\n","  def __init__(self, dim, dropout):\n","      super().__init__()\n","      self.drop = nn.Dropout(dropout)\n","      self.norm = nn.LayerNorm(dim)\n","\n","  def forward(self, x, sublayer):\n","      return x + self.drop(sublayer(self.norm(x)))\n","\n","# I simply let the model learn the positional embeddings in this notebook, since this \n","# almost produces identital results as using sin/cosin functions embeddings, as claimed\n","# in the original transformer paper. Note also that in the original paper, they multiplied \n","# the token embeddings by a factor of sqrt(d_embed), which I do not do here. \n","\n","class Encoder(nn.Module):\n","    '''Encoder = token embedding + positional embedding -> a stack of N EncoderBlock -> layer norm'''\n","    def __init__(self, config):\n","        super().__init__()\n","        self.d_embed = config.d_embed\n","        self.tok_embed = nn.Embedding(config.encoder_vocab_size, config.d_embed) \n","        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed)) \n","        self.encoder_blocks = nn.ModuleList([EncoderBlock(config) for _ in range(config.N_encoder)])\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.norm = nn.LayerNorm(config.d_embed)\n","\n","    def forward(self, input, mask=None):\n","        x = self.tok_embed(input)\n","        x_pos = self.pos_embed[:, :x.size(1), :]\n","        x = self.dropout(x + x_pos)\n","        for layer in self.encoder_blocks:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","\n","class EncoderBlock(nn.Module):\n","    '''EncoderBlock: self-attention -> positionalwise fully connected feed-forward layer'''\n","    def __init__(self, config):\n","        super(EncoderBlock, self).__init__()\n","        self.atten = MultiHeadedAttention(config.h, config.d_embed, config.dropout)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(config.d_embed, config.d_ff),\n","            nn.ReLU(),\n","            nn.Dropout(config.dropout),\n","            nn.Linear(config.d_ff, config.d_embed)\n","        )\n","        self.residual1 = ResidualConnection(config.d_embed, config.dropout)\n","        self.residual2 = ResidualConnection(config.d_embed, config.dropout)\n","\n","    def forward(self, x, mask=None):\n","        # self-attention\n","        x = self.residual1(x, lambda x: self.atten(x, x, x, mask=mask))\n","        # positionwise fully connected feed-forward layer\n","        return self.residual2(x, self.feed_forward)\n","\n","\n","class Decoder(nn.Module):\n","    '''Decoder = token embedding + positional embedding -> a stack of N DecoderBlock -> fully-connected layer'''\n","    def __init__(self, config):\n","        super().__init__()\n","        self.d_embed = config.d_embed\n","        self.tok_embed = nn.Embedding(config.decoder_vocab_size, config.d_embed)\n","        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed)) \n","        self.dropout = nn.Dropout(config.dropout)\n","        self.decoder_blocks = nn.ModuleList([DecoderBlock(config) for _ in range(config.N_decoder)])\n","        self.norm = nn.LayerNorm(config.d_embed)\n","        self.linear = nn.Linear(config.d_embed, config.decoder_vocab_size)\n","    \n","    def future_mask(self, seq_len):\n","        '''mask out tokens at future positions'''\n","        mask = (torch.triu(torch.ones(seq_len, seq_len, requires_grad=False), diagonal=1)!=0).to(DEVICE)\n","        return mask.view(1, 1, seq_len, seq_len)\n","\n","    def forward(self, memory, src_mask, trg, trg_pad_mask):\n","        seq_len = trg.size(1)\n","        trg_mask = torch.logical_or(trg_pad_mask, self.future_mask(seq_len))\n","        x = self.tok_embed(trg) + self.pos_embed[:, :trg.size(1), :]\n","        x = self.dropout(x)\n","        for layer in self.decoder_blocks:\n","            x = layer(memory, src_mask, x, trg_mask)\n","        x = self.norm(x)\n","        logits = self.linear(x)\n","        return logits\n","\n","\n","class DecoderBlock(nn.Module):\n","    ''' EncoderBlock: self-attention -> positionwise feed-forward (fully connected) layer'''\n","    def __init__(self, config):\n","        super().__init__()\n","        self.atten1 = MultiHeadedAttention(config.h, config.d_embed)\n","        self.atten2 = MultiHeadedAttention(config.h, config.d_embed)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(config.d_embed, config.d_ff),\n","            nn.ReLU(),\n","            nn.Dropout(config.dropout),\n","            nn.Linear(config.d_ff, config.d_embed)\n","        )\n","        self.residuals = nn.ModuleList([ResidualConnection(config.d_embed, config.dropout) \n","                                       for i in range(3)])\n","\n","    def forward(self, memory, src_mask, decoder_layer_input, trg_mask):\n","        x = memory\n","        y = decoder_layer_input\n","        y = self.residuals[0](y, lambda y: self.atten1(y, y, y, mask=trg_mask))\n","        # keys and values are from the encoder output\n","        y = self.residuals[1](y, lambda y: self.atten2(y, x, x, mask=src_mask))\n","        return self.residuals[2](y, self.feed_forward)\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, src_mask, trg, trg_pad_mask):\n","        return self.decoder(self.encoder(src, src_mask), src_mask, trg, trg_pad_mask)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"AWvU3VeM5ToD","executionInfo":{"status":"ok","timestamp":1650208885985,"user_tz":240,"elapsed":3,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}}},{"cell_type":"code","source":["@dataclass\n","class ModelConfig:\n","    encoder_vocab_size: int\n","    decoder_vocab_size: int\n","    d_embed: int \n","    # d_ff is the dimension of the fully-connected layer\n","    d_ff: int\n","    # h is the number of attention head\n","    h: int\n","    N_encoder: int\n","    N_decoder: int\n","    max_seq_len: int\n","    dropout: float\n","\n","def make_model(config):\n","    model = Transformer(Encoder(config), Decoder(config)).to(DEVICE)\n","\n","    # initialize model parameters\n","    # it seems that this initialization is very important!\n","    for p in model.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","    return model"],"metadata":{"id":"gxYYnrC-FAgI","executionInfo":{"status":"ok","timestamp":1650208886244,"user_tz":240,"elapsed":6,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["# Training and evaluation helper functions"],"metadata":{"id":"V7Xdzd9x8z-C"}},{"cell_type":"code","execution_count":50,"outputs":[],"source":["def make_batch_input(x, y):\n","        src = x.to(DEVICE)\n","        trg_in = y[:, :-1].to(DEVICE)\n","        trg_out = y[:, 1:].contiguous().view(-1).to(DEVICE)\n","        src_pad_mask = (src == PAD).view(src.size(0), 1, 1, src.size(-1))\n","        trg_pad_mask = (trg_in == PAD).view(trg_in.size(0), 1, 1, trg_in.size(-1))\n","        return src, trg_in, trg_out, src_pad_mask, trg_pad_mask"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"dUsGrq6fOKbC","executionInfo":{"status":"ok","timestamp":1650208886244,"user_tz":240,"elapsed":6,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}}},{"cell_type":"code","execution_count":51,"outputs":[],"source":["def train_epoch(model, dataloaders):\n","    model.train()\n","    grad_norm_clip = 1.0\n","    losses, acc, count = [], 0, 0\n","    num_batches = len(dataloaders.train_loader)\n","    pbar = tqdm(enumerate(dataloaders.train_loader), total=num_batches)\n","    for idx, (x, y)  in  pbar:\n","        optimizer.zero_grad()\n","        src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x,y)\n","        pred = model(src, src_pad_mask, trg_in, trg_pad_mask).to(DEVICE)\n","        pred = pred.view(-1, pred.size(-1))\n","        loss = loss_fn(pred, trg_out).to(DEVICE)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n","        optimizer.step()\n","        scheduler.step()\n","        losses.append(loss.item())\n","        # report progress\n","        if idx>0 and idx%50 == 0:\n","            pbar.set_description(f'ep: {scheduler.last_epoch//num_batches}, train loss={loss.item():.3f}, lr={scheduler.get_last_lr()[0]:.5f}')\n","    return np.mean(losses)\n","\n","\n","def train(model, dataloaders, epochs):\n","    global early_stop_count\n","    best_valid_loss = float('inf')\n","    train_size = len(dataloaders.train_loader)*batch_size\n","    for ep in range(epochs):\n","        train_loss = train_epoch(model, dataloaders)\n","        valid_loss = validate(model, dataloaders.valid_loader)\n","        \n","        print(f'ep: {ep}: train_loss, {train_loss:.5f}, valid_loss: {valid_loss:.5f}')\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","        else:\n","            if scheduler.last_epoch>2*warmup_steps:\n","                early_stop_count -= 1\n","                if early_stop_count<=0:   \n","                    return train_loss, valid_loss\n","    return train_loss, valid_loss\n","      \n","               \n","def validate(model, dataloder):\n","    'compute the validation loss'\n","    model.eval()\n","    losses = 0\n","    with torch.no_grad():\n","        for i, (x, y) in enumerate(dataloder):\n","            src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x,y)\n","            pred = model(src, src_pad_mask, trg_in, trg_pad_mask).to(DEVICE)\n","            pred = pred.view(-1, pred.size(-1))\n","            losses += (loss_fn(pred, trg_out).item())\n","    return losses/len(dataloder)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"T51zR33b5ToF","executionInfo":{"status":"ok","timestamp":1650208886244,"user_tz":240,"elapsed":5,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}}},{"cell_type":"code","execution_count":52,"outputs":[],"source":["def translate(model, x):\n","    'translate source sentences into the target language, without looking at the answer'\n","    with torch.no_grad():\n","        dB = x.size(0)\n","        y = torch.tensor([[BOS]*dB]).view(dB, 1).to(DEVICE)\n","        x_pad_mask = (x == PAD).view(x.size(0), 1, 1, x.size(-1)).to(DEVICE)\n","        memory = model.encoder(x, x_pad_mask)\n","        for i in range(max_seq_len):\n","            y_pad_mask = (y == PAD).view(y.size(0), 1, 1, y.size(-1)).to(DEVICE)\n","            logits = model.decoder(memory, x_pad_mask, y, y_pad_mask)\n","            last_output = logits.argmax(-1)[:, -1]\n","            last_output = last_output.view(dB, 1)\n","            y = torch.cat((y, last_output), 1).to(DEVICE)\n","    return y"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"BSUApZ2-5ToH","executionInfo":{"status":"ok","timestamp":1650208886245,"user_tz":240,"elapsed":6,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}}},{"cell_type":"code","source":["def remove_pad(sent):\n","    '''truncate the sentence if BOS is in it,\n","     otherwise simply remove the padding tokens at the end'''\n","    if sent.count(EOS)>0:\n","      sent = sent[0:sent.index(EOS)+1]\n","    while sent and sent[-1] == PAD:\n","            sent = sent[:-1]\n","    return sent\n","\n","def decode_sentence(detokenizer, sentence_ids):\n","    'convert a tokenized sentence (a list of numbers) to a literal string'\n","    if not isinstance(sentence_ids, list):\n","        sentence_ids = sentence_ids.tolist()\n","    sentence_ids = remove_pad(sentence_ids)\n","    return detokenizer(sentence_ids).replace(\"<bos>\", \"\")\\\n","           .replace(\"<eos>\", \"\").strip().replace(\" .\", \".\")\n","\n","def evaluate(model, dataloader, num_batch=None):\n","    'evaluate the model, and compute the BLEU score'\n","    model.eval()\n","    refs, cans, bleus = [], [], []\n","    with torch.no_grad():\n","        for idx, (x, y) in enumerate(dataloader):\n","            src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x,y)\n","            translation = translate(model, src)\n","            trg_out = trg_out.view(x.size(0), -1)\n","            refs = refs + [decode_sentence(detokenizers[TRG], trg_out[i]) for i in range(len(src))]\n","            cans = cans + [decode_sentence(detokenizers[TRG], translation[i]) for i in range(len(src))] \n","            if num_batch and idx>=num_batch:\n","                break\n","        bleus.append(sacrebleu.corpus_bleu(cans, [refs]).score)\n","        # print some examples\n","        for i in range(3):\n","            print(f'src:  {decode_sentence(detokenizers[SRC], src[i])}')\n","            print(f'trg:  {decode_sentence(detokenizers[TRG], trg_out[i])}')\n","            print(f'pred: {decode_sentence(detokenizers[TRG], translation[i])}')\n","        return np.mean(bleus)"],"metadata":{"id":"zQtU_WoLPMwM","executionInfo":{"status":"ok","timestamp":1650208886245,"user_tz":240,"elapsed":5,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["config = ModelConfig(encoder_vocab_size = vocab_sizes[SRC], \n","                     decoder_vocab_size=vocab_sizes[TRG],\n","                     d_embed=512, \n","                     d_ff=512, \n","                     h=8,\n","                     N_encoder=3, \n","                     N_decoder=3, \n","                     max_seq_len=max_seq_len,\n","                     dropout=0.1\n","                     )\n","\n","data_loaders = Dataloaders()\n","train_size = len(data_loaders.train_loader)*batch_size\n","model = make_model(config)\n","model_size = sum([p.numel() for p in model.parameters()])\n","print(f'model_size: {model_size}, train_set_size: {train_size}')\n","warmup_steps = 3*len(data_loaders.train_loader)\n","# lr first increases in the warmup steps, and then descreases\n","lr_fn = lambda step: config.d_embed**(-0.5) * min([(step+1)**(-0.5), (step+1)*warmup_steps**(-1.5)])\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_fn)\n","loss_fn = nn.CrossEntropyLoss(ignore_index=PAD)\n","early_stop_count = 2\n","train_loss, valid_loss = train(model, data_loaders, epochs=10)\n","test_loss  = validate(model, data_loaders.test_loader)\n","\n","print(\"train set examples:\")\n","train_bleu = evaluate(model, data_loaders.train_loader, 20)\n","print(\"validation set examples:\")\n","valid_bleu = evaluate(model, data_loaders.valid_loader)\n","print(\"test set examples:\")\n","test_bleu  = evaluate(model, data_loaders.test_loader)\n","print(f'train_loss: {train_loss:.4f}, valid_loss: {valid_loss:.4f}, test_loss: {test_loss:.4f}')\n","print(f'test_bleu: {test_bleu:.4f}, valid_bleu: {valid_bleu:.4f} train_bleu: {train_bleu:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-HOapnPDtdZ","executionInfo":{"status":"ok","timestamp":1650209386501,"user_tz":240,"elapsed":283468,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"d9dd2ade-69a2-4061-d8e9-c0b62ab453c0"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["model_size: 26201096, train_set_size: 29056\n"]},{"output_type":"stream","name":"stderr","text":["ep: 0, train loss=3.977, lr=0.00025: 100%|██████████| 227/227 [00:30<00:00,  7.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 0: train_loss, 5.55964, valid_loss: 3.72496\n"]},{"output_type":"stream","name":"stderr","text":["ep: 1, train loss=2.712, lr=0.00053: 100%|██████████| 227/227 [00:29<00:00,  7.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 1: train_loss, 3.14020, valid_loss: 2.65102\n"]},{"output_type":"stream","name":"stderr","text":["ep: 2, train loss=2.315, lr=0.00082: 100%|██████████| 227/227 [00:30<00:00,  7.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 2: train_loss, 2.27996, valid_loss: 2.18214\n"]},{"output_type":"stream","name":"stderr","text":["ep: 3, train loss=1.557, lr=0.00074: 100%|██████████| 227/227 [00:30<00:00,  7.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 3: train_loss, 1.76503, valid_loss: 1.94196\n"]},{"output_type":"stream","name":"stderr","text":["ep: 4, train loss=1.389, lr=0.00066: 100%|██████████| 227/227 [00:30<00:00,  7.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 4: train_loss, 1.37548, valid_loss: 1.85112\n"]},{"output_type":"stream","name":"stderr","text":["ep: 5, train loss=1.024, lr=0.00060: 100%|██████████| 227/227 [00:30<00:00,  7.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 5: train_loss, 1.11018, valid_loss: 1.83961\n"]},{"output_type":"stream","name":"stderr","text":["ep: 6, train loss=0.978, lr=0.00056: 100%|██████████| 227/227 [00:30<00:00,  7.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 6: train_loss, 0.91141, valid_loss: 1.88259\n"]},{"output_type":"stream","name":"stderr","text":["ep: 7, train loss=0.850, lr=0.00052: 100%|██████████| 227/227 [00:30<00:00,  7.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 7: train_loss, 0.75758, valid_loss: 1.92926\n","train set examples:\n","src:  Zwei Hunde haben beide denselben Stock im Maul und schwimmen.\n","trg:  Two dogs both have the same stick in their mouth while they are swimming.\n","pred: Two dogs both have the same stick in their mouth and are swimming.\n","src:  Die Jungen und der alte Mann hacken Holz zum Aufstapeln.\n","trg:  The boys and the old man are splitting wood to stack.\n","pred: The boys and the old man are stacking wood to stack.\n","src:  Eine für das US-amerikanische Volleyballteam spielende Frau schmettert den Ball über das Netz.\n","trg:  A woman is playing for the  ⁇ SA volleyball team, spiking the ball over the net.\n","pred: A woman playing the  ⁇ S of a volleyball team, spiking the net over the net.\n","validation set examples:\n","src:  Ein Mann leckt das Gesicht einer Frau mit Brille ab.\n","trg:  A man licking the face of a woman wearing glasses.\n","pred: A man licks his face with glasses.\n","src:  Ein Mann mit Brille und ein kleines Mädchen mit einer Schwimmweste schwimmen in einem Pool.\n","trg:  A man with glasses and a little girl with a life jacket swimming in a pool.\n","pred: A man with glasses and a young girl in a pool with a life jacket.\n","src:  Ein Mann im weißen Hemd macht Schaschlik.\n","trg:  A man in a white shirt is making shish kabob.\n","pred: A man in a white shirt is about to take off the job.\n","test set examples:\n","src:  Frau mit Kamera wirft ihrem braunen Hund einen Frisbee zum Fangen zu.\n","trg:  Woman with camera is throwing a Frisbee for her brown dog to catch.\n","pred: Woman throwing her brown dog to catch a Frisbee.\n","src:  Eine Mann mit einem Stirnband steht auf der Straße vor seinen Sachen.\n","trg:  A person in a bandanna stands on the street in front his things.\n","pred: A man wearing a headband is standing in front of the street with many things.\n","src:  Ein Mann in einem Anzug und mit Hut spielt auf der Straße Gitarre.\n","trg:  A man in a suit and hat is playing the guitar on the street.\n","pred: A man in a suit and hat plays guitar on the street.\n","train_loss: 0.7576, valid_loss: 1.9293, test_loss: 1.9764\n","test_bleu: 34.8799, valid_bleu: 34.2526 train_bleu: 58.5784\n"]}]},{"cell_type":"markdown","source":["So with a transformer model of 26 million parameters, trained on a training set of 29k sentence pairs, we got a test BLEU score of 34.88."],"metadata":{"id":"SE7RdQ57HN9J"}},{"cell_type":"code","source":["def translate_this_sentence(text: str):\n","    'translate the source sentence in string formate into target language'\n","    input = torch.tensor([[BOS] + tokenizers[SRC](text) + [EOS]]).to(DEVICE)\n","    output = translate(model, input)\n","    return decode_sentence(detokenizers[TRG], output[0])\n","\n","translate_this_sentence(\"Eine Gruppe von Menschen steht vor einem Iglu.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"6DHwo3xcYmXy","executionInfo":{"status":"ok","timestamp":1650209454744,"user_tz":240,"elapsed":516,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"1ef2dac5-b165-45cc-92c7-ca038f9ecc24"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'A group of people are standing in front of an igloo.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":58}]}]}