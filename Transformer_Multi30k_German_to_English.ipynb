{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer_Multi30k_German_to_English.ipynb","provenance":[{"file_id":"1bU3hHSQGY-MfTgfZaQWQzivJD__STGOR","timestamp":1650031322821},{"file_id":"1YhqP5st0yiBt4FJofOwY7g6GrzGZWQNW","timestamp":1649105742866},{"file_id":"1P7oU2EWQ1Qk17N9NutZv9FV1a_hMGUoR","timestamp":1647373127550}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["In this notebook, I train a transformer model for translating German to English. The model structure is the same as that of the original [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) paper."],"metadata":{"id":"D23TRFotJ6bh"}},{"cell_type":"code","source":["!pip install sentencepiece --quiet\n","!pip install sacrebleu --quiet\n","!pip install torchdata --quiet"],"metadata":{"id":"jtTpObu_cdP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","from dataclasses import dataclass\n","\n","import numpy as np\n","import sacrebleu\n","import sentencepiece as spm\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torchtext.datasets import Multi30k\n","from tqdm import tqdm\n","\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)"],"metadata":{"id":"ToFDXgFP5fys","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650937537690,"user_tz":240,"elapsed":588,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"00a2ca85-4ea1-4d86-b809-1936b84d4ead"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["SRC = \"de\"\n","TRG = \"en\""],"metadata":{"id":"tlCncOUK4lUo","executionInfo":{"status":"ok","timestamp":1650937538850,"user_tz":240,"elapsed":168,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Get German and English tokenizers from SentencePiece"],"metadata":{"id":"Jq10RHNKZTDo"}},{"cell_type":"code","source":["train_iter = Multi30k(split='train', language_pair=(SRC, TRG))\n","f_de = open(\"Multi30k_de_text.txt\", \"w\")\n","f_en = open(\"Multi30k_en_text.txt\", \"w\")\n","for pair in train_iter:\n","    f_de.write(pair[0]+'\\n')\n","    f_en.write(pair[1]+'\\n')\n","f_de.close()\n","f_en.close()"],"metadata":{"id":"hz9u_PmSdLdc","executionInfo":{"status":"ok","timestamp":1650937543309,"user_tz":240,"elapsed":3757,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["en_vocab_size = 8200\n","de_vocab_size = 10000\n","vocab_sizes = {\"en\": en_vocab_size, \"de\": de_vocab_size}"],"metadata":{"id":"N1eDUeBkFnea","executionInfo":{"status":"ok","timestamp":1650937546040,"user_tz":240,"elapsed":115,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# train sentencepiece models to get tokenizers\n","spm.SentencePieceTrainer.train\\\n","(f'--input=Multi30k_de_text.txt --model_prefix=Multi30k_de --user_defined_symbols=<pad> --vocab_size={de_vocab_size}')\n","spm.SentencePieceTrainer.train\\\n","(f'--input=Multi30k_en_text.txt --model_prefix=Multi30k_en --user_defined_symbols=<pad> --vocab_size={en_vocab_size}')\n","\n","# make SentencePieceProcessor instances and load the model files\n","de_sp = spm.SentencePieceProcessor()\n","de_sp.load('Multi30k_de.model')\n","en_sp = spm.SentencePieceProcessor()\n","en_sp.load('Multi30k_en.model')\n","\n","tokenizers = {\"en\": en_sp.encode_as_ids, \"de\": de_sp.encode_as_ids}\n","detokenizers = {\"en\":en_sp.decode_ids, \"de\":de_sp.decode_ids}\n","\n","# encode: text => id\n","print(en_sp.encode_as_pieces('This is a test'))\n","print(en_sp.encode_as_ids('This is a test'))\n","\n","# decode: id => text\n","print(en_sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n","print(en_sp.decode_ids([302, 258, 10, 4, 2395]))"],"metadata":{"id":"asaHCE3IZhrl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650937549032,"user_tz":240,"elapsed":973,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"cb8e5f82-bde5-40c8-da14-4a5a696e3bbc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['▁Th', 'is', '▁is', '▁a', '▁test']\n","[302, 258, 10, 4, 2395]\n","▁This is a test\n","This is a test\n"]}]},{"cell_type":"code","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['<unk>', '<s>', '</s>', '<pad>', '▁a', '.', '▁A', '▁in', '▁the', '▁on', '▁is', '▁man', '▁and', '▁of', '▁with', 's', 'ing', '▁', ',', '▁woman']\n","['<unk>', '<s>', '</s>', '<pad>', '.', '▁eine', '▁Ein', 'm', '▁in', '▁mit', ',', '▁und', '▁auf', '▁ein', '▁Mann', '▁einer', '▁Eine', 'n', '▁der', '▁Frau']\n"]}],"source":["print([en_sp.id_to_piece(id) for id in range(20)])\n","print([de_sp.id_to_piece(id) for id in range(20)])"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"zv9Q9YyAUqoh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650937550417,"user_tz":240,"elapsed":119,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"e73700d1-1b28-46bd-98b3-09ea4f84746c"}},{"cell_type":"code","source":["# indexes of special symbols\n","UNK, BOS, EOS, PAD = 0, 1, 2, 3"],"metadata":{"id":"8grAiF0GfmYx","executionInfo":{"status":"ok","timestamp":1650937551487,"user_tz":240,"elapsed":195,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Data processing"],"metadata":{"collapsed":false,"id":"OuOHh7fF5Tn8"}},{"cell_type":"code","source":["train_iter = Multi30k(split='train', language_pair=(SRC, TRG))\n","valid_iter = Multi30k(split='valid', language_pair=(SRC, TRG))\n","test_iter  = Multi30k(split='test',  language_pair=(SRC, TRG))\n","\n","train_set = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in train_iter if x!='']\n","valid_set = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in valid_iter if x!='']\n","test_set  = [(x.rstrip('\\n'), y.rstrip('\\n')) for x, y in test_iter if x!='']\n","print(len(train_set), len(valid_set), len(test_set))\n","for i in range(10):\n","   print(train_set[i])"],"metadata":{"id":"jh3Nxm7VaHIQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650937555460,"user_tz":240,"elapsed":2571,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"a6f5f56d-1299-4add-9353-ca86bbd8ddb6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:181: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n","  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["29000 1014 1000\n","('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.', 'Two young, White males are outside near many bushes.')\n","('Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.', 'Several men in hard hats are operating a giant pulley system.')\n","('Ein kleines Mädchen klettert in ein Spielhaus aus Holz.', 'A little girl climbing into a wooden playhouse.')\n","('Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.', 'A man in a blue shirt is standing on a ladder cleaning a window.')\n","('Zwei Männer stehen am Herd und bereiten Essen zu.', 'Two men are at the stove preparing food.')\n","('Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.', 'A man in green holds a guitar while the other man observes his shirt.')\n","('Ein Mann lächelt einen ausgestopften Löwen an.', 'A man is smiling at a stuffed lion')\n","('Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.', 'A trendy girl talking on her cellphone while gliding slowly down the street.')\n","('Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.', 'A woman with a large purse is walking by a gate.')\n","('Jungen tanzen mitten in der Nacht auf Pfosten.', 'Boys dancing on poles in the middle of the night.')\n"]}]},{"cell_type":"code","execution_count":10,"outputs":[],"source":["max_seq_len = 50\n","def tokenize_dataset(dataset):\n","    'tokenize a dataset and add [BOS] and [EOS] to the beginning and end of the sentences'\n","    return [(torch.tensor([BOS]+tokenizers[SRC](src_text)[0:max_seq_len-2]+[EOS]),\n","             torch.tensor([BOS]+tokenizers[TRG](trg_text)[0:max_seq_len-2]+[EOS]))\n","            for src_text, trg_text in dataset]\n","          \n","train_tokenized = tokenize_dataset(train_set)\n","valid_tokenized = tokenize_dataset(valid_set)\n","test_tokenized  = tokenize_dataset(test_set)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"gF5sy0p65ToA","executionInfo":{"status":"ok","timestamp":1650937565372,"user_tz":240,"elapsed":1233,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}}},{"cell_type":"code","source":["class TranslationDataset(Dataset):\n","    'create a dataset for torch.utils.data.DataLoader() '\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","\n","def pad_sequence(batch):\n","    'collate function for padding sentences such that all \\\n","    the sentences in the batch have the same length'\n","    src_seqs  = [src for src, trg in batch]\n","    trg_seqs  = [trg for src, trg in batch]\n","    src_padded = torch.nn.utils.rnn.pad_sequence(src_seqs,\n","                                batch_first=True, padding_value = PAD)\n","    trg_padded = torch.nn.utils.rnn.pad_sequence(trg_seqs,\n","                                batch_first=True, padding_value = PAD)\n","    return src_padded, trg_padded\n"],"metadata":{"id":"8RSKKEGTHICe","executionInfo":{"status":"ok","timestamp":1650937565372,"user_tz":240,"elapsed":2,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["batch_size = 128\n","\n","class Dataloaders:\n","    'Dataloaders contains train_loader, test_loader and valid_loader for training and evaluation '\n","    def __init__(self):\n","        self.train_dataset = TranslationDataset(train_tokenized)\n","        self.valid_dataset = TranslationDataset(valid_tokenized)\n","        self.test_dataset  = TranslationDataset(test_tokenized)\n","        \n","        # each batch returned by dataloader will be padded such that all the texts in\n","        # that batch have the same length as the longest text in that batch\n","        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size,\n","                                                shuffle=True, collate_fn = pad_sequence)\n","        \n","        self.test_loader = torch.utils.data.DataLoader(self.test_dataset, batch_size=batch_size,\n","                                                shuffle=True, collate_fn=pad_sequence)\n","        \n","        self.valid_loader = torch.utils.data.DataLoader(self.valid_dataset, batch_size=batch_size,\n","                                                shuffle=True, collate_fn=pad_sequence)\n"],"metadata":{"id":"WgeG2xQwFmIZ","executionInfo":{"status":"ok","timestamp":1650937567516,"user_tz":240,"elapsed":220,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Transformer Model"],"metadata":{"id":"6t5Bkt_LOu0p"}},{"cell_type":"code","execution_count":13,"outputs":[],"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_embed, dropout=0.0):\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_embed % h == 0 # check the h number\n","        self.d_k = d_embed//h\n","        self.d_embed = d_embed\n","        self.h = h\n","        self.WQ = nn.Linear(d_embed, d_embed)\n","        self.WK = nn.Linear(d_embed, d_embed)\n","        self.WV = nn.Linear(d_embed, d_embed)\n","        self.linear = nn.Linear(d_embed, d_embed)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x_query, x_key, x_value, mask=None):\n","        nbatch = x_query.size(0) # get batch size\n","        # 1) Linear projections to get the multi-head query, key and value tensors\n","        # x_query, x_key, x_value dimension: nbatch * seq_len * d_embed\n","        # LHS query, key, value dimensions: nbatch * h * seq_len * d_k\n","        query = self.WQ(x_query).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n","        key   = self.WK(x_key).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n","        value = self.WV(x_value).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n","        # 2) Attention\n","        # scores has dimensions: nbatch * h * seq_len * seq_len\n","        scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.d_k)\n","        # 3) Mask out padding tokens and future tokens\n","        if mask is not None:\n","            scores = scores.masked_fill(mask, float('-inf'))\n","        # p_atten dimensions: nbatch * h * seq_len * seq_len\n","        p_atten = torch.nn.functional.softmax(scores, dim=-1)\n","        p_atten = self.dropout(p_atten)\n","        # x dimensions: nbatch * h * seq_len * d_k\n","        x = torch.matmul(p_atten, value)\n","        # x now has dimensions:nbtach * seq_len * d_embed\n","        x = x.transpose(1, 2).contiguous().view(nbatch, -1, self.d_embed)\n","        return self.linear(x) # final linear layer\n","\n","\n","class ResidualConnection(nn.Module):\n","  '''residual connection: x + dropout(sublayer(layernorm(x))) '''\n","  def __init__(self, dim, dropout):\n","      super().__init__()\n","      self.drop = nn.Dropout(dropout)\n","      self.norm = nn.LayerNorm(dim)\n","\n","  def forward(self, x, sublayer):\n","      return x + self.drop(sublayer(self.norm(x)))\n","\n","# I simply let the model learn the positional embeddings in this notebook, since this \n","# almost produces identital results as using sin/cosin functions embeddings, as claimed\n","# in the original transformer paper. Note also that in the original paper, they multiplied \n","# the token embeddings by a factor of sqrt(d_embed), which I do not do here. \n","\n","class Encoder(nn.Module):\n","    '''Encoder = token embedding + positional embedding -> a stack of N EncoderBlock -> layer norm'''\n","    def __init__(self, config):\n","        super().__init__()\n","        self.d_embed = config.d_embed\n","        self.tok_embed = nn.Embedding(config.encoder_vocab_size, config.d_embed) \n","        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed)) \n","        self.encoder_blocks = nn.ModuleList([EncoderBlock(config) for _ in range(config.N_encoder)])\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.norm = nn.LayerNorm(config.d_embed)\n","\n","    def forward(self, input, mask=None):\n","        x = self.tok_embed(input)\n","        x_pos = self.pos_embed[:, :x.size(1), :]\n","        x = self.dropout(x + x_pos)\n","        for layer in self.encoder_blocks:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","\n","class EncoderBlock(nn.Module):\n","    '''EncoderBlock: self-attention -> position-wise fully connected feed-forward layer'''\n","    def __init__(self, config):\n","        super(EncoderBlock, self).__init__()\n","        self.atten = MultiHeadedAttention(config.h, config.d_embed, config.dropout)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(config.d_embed, config.d_ff),\n","            nn.ReLU(),\n","            nn.Dropout(config.dropout),\n","            nn.Linear(config.d_ff, config.d_embed)\n","        )\n","        self.residual1 = ResidualConnection(config.d_embed, config.dropout)\n","        self.residual2 = ResidualConnection(config.d_embed, config.dropout)\n","\n","    def forward(self, x, mask=None):\n","        # self-attention\n","        x = self.residual1(x, lambda x: self.atten(x, x, x, mask=mask))\n","        # position-wise fully connected feed-forward layer\n","        return self.residual2(x, self.feed_forward)\n","\n","\n","class Decoder(nn.Module):\n","    '''Decoder = token embedding + positional embedding -> a stack of N DecoderBlock -> fully-connected layer'''\n","    def __init__(self, config):\n","        super().__init__()\n","        self.d_embed = config.d_embed\n","        self.tok_embed = nn.Embedding(config.decoder_vocab_size, config.d_embed)\n","        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed)) \n","        self.dropout = nn.Dropout(config.dropout)\n","        self.decoder_blocks = nn.ModuleList([DecoderBlock(config) for _ in range(config.N_decoder)])\n","        self.norm = nn.LayerNorm(config.d_embed)\n","        self.linear = nn.Linear(config.d_embed, config.decoder_vocab_size)\n","    \n","    def future_mask(self, seq_len):\n","        '''mask out tokens at future positions'''\n","        mask = (torch.triu(torch.ones(seq_len, seq_len, requires_grad=False), diagonal=1)!=0).to(DEVICE)\n","        return mask.view(1, 1, seq_len, seq_len)\n","\n","    def forward(self, memory, src_mask, trg, trg_pad_mask):\n","        seq_len = trg.size(1)\n","        trg_mask = torch.logical_or(trg_pad_mask, self.future_mask(seq_len))\n","        x = self.tok_embed(trg) + self.pos_embed[:, :trg.size(1), :]\n","        x = self.dropout(x)\n","        for layer in self.decoder_blocks:\n","            x = layer(memory, src_mask, x, trg_mask)\n","        x = self.norm(x)\n","        logits = self.linear(x)\n","        return logits\n","\n","\n","class DecoderBlock(nn.Module):\n","    ''' EncoderBlock: self-attention -> position-wise feed-forward (fully connected) layer'''\n","    def __init__(self, config):\n","        super().__init__()\n","        self.atten1 = MultiHeadedAttention(config.h, config.d_embed)\n","        self.atten2 = MultiHeadedAttention(config.h, config.d_embed)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(config.d_embed, config.d_ff),\n","            nn.ReLU(),\n","            nn.Dropout(config.dropout),\n","            nn.Linear(config.d_ff, config.d_embed)\n","        )\n","        self.residuals = nn.ModuleList([ResidualConnection(config.d_embed, config.dropout) \n","                                       for i in range(3)])\n","\n","    def forward(self, memory, src_mask, decoder_layer_input, trg_mask):\n","        x = memory\n","        y = decoder_layer_input\n","        y = self.residuals[0](y, lambda y: self.atten1(y, y, y, mask=trg_mask))\n","        # keys and values are from the encoder output\n","        y = self.residuals[1](y, lambda y: self.atten2(y, x, x, mask=src_mask))\n","        return self.residuals[2](y, self.feed_forward)\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, src_mask, trg, trg_pad_mask):\n","        return self.decoder(self.encoder(src, src_mask), src_mask, trg, trg_pad_mask)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"AWvU3VeM5ToD","executionInfo":{"status":"ok","timestamp":1650937568886,"user_tz":240,"elapsed":317,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}}},{"cell_type":"code","source":["@dataclass\n","class ModelConfig:\n","    encoder_vocab_size: int\n","    decoder_vocab_size: int\n","    d_embed: int\n","    # d_ff is the dimension of the fully-connected  feed-forward layer\n","    d_ff: int\n","    # h is the number of attention head\n","    h: int\n","    N_encoder: int\n","    N_decoder: int\n","    max_seq_len: int\n","    dropout: float\n","\n","def make_model(config):\n","    model = Transformer(Encoder(config), Decoder(config)).to(DEVICE)\n","\n","    # initialize model parameters\n","    # it seems that this initialization is very important!\n","    for p in model.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","    return model"],"metadata":{"id":"gxYYnrC-FAgI","executionInfo":{"status":"ok","timestamp":1650937573176,"user_tz":240,"elapsed":107,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Training and evaluation helper functions"],"metadata":{"id":"V7Xdzd9x8z-C"}},{"cell_type":"code","execution_count":15,"outputs":[],"source":["def make_batch_input(x, y):\n","        src = x.to(DEVICE)\n","        trg_in = y[:, :-1].to(DEVICE)\n","        trg_out = y[:, 1:].contiguous().view(-1).to(DEVICE)\n","        src_pad_mask = (src == PAD).view(src.size(0), 1, 1, src.size(-1))\n","        trg_pad_mask = (trg_in == PAD).view(trg_in.size(0), 1, 1, trg_in.size(-1))\n","        return src, trg_in, trg_out, src_pad_mask, trg_pad_mask"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"dUsGrq6fOKbC","executionInfo":{"status":"ok","timestamp":1650937576614,"user_tz":240,"elapsed":161,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}}},{"cell_type":"code","execution_count":16,"outputs":[],"source":["from numpy.lib.utils import lookfor\n","def train_epoch(model, dataloaders):\n","    model.train()\n","    grad_norm_clip = 1.0\n","    losses, acc, count = [], 0, 0\n","    num_batches = len(dataloaders.train_loader)\n","    pbar = tqdm(enumerate(dataloaders.train_loader), total=num_batches)\n","    for idx, (x, y)  in  pbar:\n","        optimizer.zero_grad()\n","        src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x,y)\n","        pred = model(src, src_pad_mask, trg_in, trg_pad_mask).to(DEVICE)\n","        pred = pred.view(-1, pred.size(-1))\n","        loss = loss_fn(pred, trg_out).to(DEVICE)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n","        optimizer.step()\n","        scheduler.step()\n","        losses.append(loss.item())\n","        # report progress\n","        if idx>0 and idx%50 == 0:\n","            pbar.set_description(f'train loss={loss.item():.3f}, lr={scheduler.get_last_lr()[0]:.5f}')\n","    return np.mean(losses)\n","\n","\n","def train(model, dataloaders, epochs):\n","    global early_stop_count\n","    best_valid_loss = float('inf')\n","    train_size = len(dataloaders.train_loader)*batch_size\n","    for ep in range(epochs):\n","        train_loss = train_epoch(model, dataloaders)\n","        valid_loss = validate(model, dataloaders.valid_loader)\n","        \n","        print(f'ep: {ep}: train_loss={train_loss:.5f}, valid_loss={valid_loss:.5f}')\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","        else:\n","            if scheduler.last_epoch>2*warmup_steps:\n","                early_stop_count -= 1\n","                if early_stop_count<=0:   \n","                    return train_loss, valid_loss\n","    return train_loss, valid_loss\n","      \n","               \n","def validate(model, dataloder):\n","    'compute the validation loss'\n","    model.eval()\n","    losses = []\n","    with torch.no_grad():\n","        for i, (x, y) in enumerate(dataloder):\n","            src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x,y)\n","            pred = model(src, src_pad_mask, trg_in, trg_pad_mask).to(DEVICE)\n","            pred = pred.view(-1, pred.size(-1))\n","            losses.append(loss_fn(pred, trg_out).item())\n","    return np.mean(losses)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"T51zR33b5ToF","executionInfo":{"status":"ok","timestamp":1650937577704,"user_tz":240,"elapsed":225,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}}},{"cell_type":"code","execution_count":17,"outputs":[],"source":["def translate(model, x):\n","    'translate source sentences into the target language, without looking at the answer'\n","    with torch.no_grad():\n","        dB = x.size(0)\n","        y = torch.tensor([[BOS]*dB]).view(dB, 1).to(DEVICE)\n","        x_pad_mask = (x == PAD).view(x.size(0), 1, 1, x.size(-1)).to(DEVICE)\n","        memory = model.encoder(x, x_pad_mask)\n","        for i in range(max_seq_len):\n","            y_pad_mask = (y == PAD).view(y.size(0), 1, 1, y.size(-1)).to(DEVICE)\n","            logits = model.decoder(memory, x_pad_mask, y, y_pad_mask)\n","            last_output = logits.argmax(-1)[:, -1]\n","            last_output = last_output.view(dB, 1)\n","            y = torch.cat((y, last_output), 1).to(DEVICE)\n","    return y"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"BSUApZ2-5ToH","executionInfo":{"status":"ok","timestamp":1650937580517,"user_tz":240,"elapsed":176,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}}},{"cell_type":"code","source":["def remove_pad(sent):\n","    '''truncate the sentence if BOS is in it,\n","     otherwise simply remove the padding tokens at the end'''\n","    if sent.count(EOS)>0:\n","      sent = sent[0:sent.index(EOS)+1]\n","    while sent and sent[-1] == PAD:\n","            sent = sent[:-1]\n","    return sent\n","\n","def decode_sentence(detokenizer, sentence_ids):\n","    'convert a tokenized sentence (a list of numbers) to a literal string'\n","    if not isinstance(sentence_ids, list):\n","        sentence_ids = sentence_ids.tolist()\n","    sentence_ids = remove_pad(sentence_ids)\n","    return detokenizer(sentence_ids).replace(\"<bos>\", \"\")\\\n","           .replace(\"<eos>\", \"\").strip().replace(\" .\", \".\")\n","\n","def evaluate(model, dataloader, num_batch=None):\n","    'evaluate the model, and compute the BLEU score'\n","    model.eval()\n","    refs, cans, bleus = [], [], []\n","    with torch.no_grad():\n","        for idx, (x, y) in enumerate(dataloader):\n","            src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x,y)\n","            translation = translate(model, src)\n","            trg_out = trg_out.view(x.size(0), -1)\n","            refs = refs + [decode_sentence(detokenizers[TRG], trg_out[i]) for i in range(len(src))]\n","            cans = cans + [decode_sentence(detokenizers[TRG], translation[i]) for i in range(len(src))] \n","            if num_batch and idx>=num_batch:\n","                break\n","        print(min([len(x) for x in refs]))\n","        bleus.append(sacrebleu.corpus_bleu(cans, [refs]).score)\n","        # print some examples\n","        for i in range(3):\n","            print(f'src:  {decode_sentence(detokenizers[SRC], src[i])}')\n","            print(f'trg:  {decode_sentence(detokenizers[TRG], trg_out[i])}')\n","            print(f'pred: {decode_sentence(detokenizers[TRG], translation[i])}')\n","        return np.mean(bleus)"],"metadata":{"id":"zQtU_WoLPMwM","executionInfo":{"status":"ok","timestamp":1650937582229,"user_tz":240,"elapsed":165,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"FtpVeM7Xzanz"}},{"cell_type":"code","source":["config = ModelConfig(encoder_vocab_size = vocab_sizes[SRC], \n","                     decoder_vocab_size=vocab_sizes[TRG],\n","                     d_embed=512, \n","                     d_ff=512, \n","                     h=8,\n","                     N_encoder=3, \n","                     N_decoder=3, \n","                     max_seq_len=max_seq_len,\n","                     dropout=0.1\n","                     )\n","\n","data_loaders = Dataloaders()\n","train_size = len(data_loaders.train_loader)*batch_size\n","model = make_model(config)\n","model_size = sum([p.numel() for p in model.parameters()])\n","print(f'model_size: {model_size}, train_set_size: {train_size}')\n","warmup_steps = 3*len(data_loaders.train_loader)\n","# lr first increases in the warmup steps, and then descreases\n","lr_fn = lambda step: config.d_embed**(-0.5) * min([(step+1)**(-0.5), (step+1)*warmup_steps**(-1.5)])\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_fn)\n","loss_fn = nn.CrossEntropyLoss(ignore_index=PAD)\n","early_stop_count = 2\n","train_loss, valid_loss = train(model, data_loaders, epochs=10)\n","test_loss  = validate(model, data_loaders.test_loader)\n","\n","print(\"train set examples:\")\n","train_bleu = evaluate(model, data_loaders.train_loader, 20)\n","print(\"validation set examples:\")\n","valid_bleu = evaluate(model, data_loaders.valid_loader)\n","print(\"test set examples:\")\n","test_bleu  = evaluate(model, data_loaders.test_loader)\n","print(f'train_loss: {train_loss:.4f}, valid_loss: {valid_loss:.4f}, test_loss: {test_loss:.4f}')\n","print(f'test_bleu: {test_bleu:.4f}, valid_bleu: {valid_bleu:.4f} train_bleu: {train_bleu:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-HOapnPDtdZ","outputId":"a9b6f577-a23c-4dde-a576-66204fb54599","executionInfo":{"status":"ok","timestamp":1650937841023,"user_tz":240,"elapsed":251218,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["model_size: 26201096, train_set_size: 29056\n"]},{"output_type":"stream","name":"stderr","text":["train loss=3.828, lr=0.00025: 100%|██████████| 227/227 [00:25<00:00,  8.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 0: train_loss=5.52948, valid_loss=3.71259\n"]},{"output_type":"stream","name":"stderr","text":["train loss=2.664, lr=0.00053: 100%|██████████| 227/227 [00:25<00:00,  8.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 1: train_loss=3.08626, valid_loss=2.55867\n"]},{"output_type":"stream","name":"stderr","text":["train loss=2.042, lr=0.00082: 100%|██████████| 227/227 [00:25<00:00,  8.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 2: train_loss=2.21475, valid_loss=2.11941\n"]},{"output_type":"stream","name":"stderr","text":["train loss=1.756, lr=0.00074: 100%|██████████| 227/227 [00:25<00:00,  8.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 3: train_loss=1.73936, valid_loss=1.88238\n"]},{"output_type":"stream","name":"stderr","text":["train loss=1.616, lr=0.00066: 100%|██████████| 227/227 [00:25<00:00,  8.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 4: train_loss=1.39079, valid_loss=1.78430\n"]},{"output_type":"stream","name":"stderr","text":["train loss=1.272, lr=0.00060: 100%|██████████| 227/227 [00:25<00:00,  8.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 5: train_loss=1.14788, valid_loss=1.78204\n"]},{"output_type":"stream","name":"stderr","text":["train loss=0.993, lr=0.00056: 100%|██████████| 227/227 [00:25<00:00,  8.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 6: train_loss=0.96272, valid_loss=1.78856\n"]},{"output_type":"stream","name":"stderr","text":["train loss=0.913, lr=0.00052: 100%|██████████| 227/227 [00:25<00:00,  8.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ep: 7: train_loss=0.81409, valid_loss=1.82965\n","train set examples:\n","21\n","src:  Eine ältere blonde Frau mit Sonnenbrille lehnt an einem sonnigen Tag auf einem Brückengeländer.\n","trg:  An older blond woman with sunglasses leaning on a bridge railing on a sunny day.\n","pred: An older blond woman with sunglasses leans on a bridge railing on a sunny day.\n","src:  Eine Frau mit einer braunen Tasche befestigt ein rosa-gelbes Bouquet an ihrem Fahrrad.\n","trg:  A woman with brown bag attaches a pink and yellow bouquet to her bike.\n","pred: A woman with a brown bag attaches a pink and yellow bouquet to her bike.\n","src:  Zwei Frauen, die sehr sportlich sind, spielen Beachvolleyball.\n","trg:  Two women, who are very athletic, are playing beach volleyball.\n","pred: Two women, who are very athletic, are playing beach volleyball.\n","validation set examples:\n","20\n","src:  Eine Frau zieht einem kleinen Mädchen einen Helm an.\n","trg:  A woman is putting a helmet on a small girl.\n","pred: A woman pulling a young girl a helmet.\n","src:  Eine Frau arbeitet am Wochenende an ihrer Terrasse.\n","trg:  A woman working on her deck on the weekend.\n","pred: A woman works on her deck at the relaxing.\n","src:  Roller-Derby-Spielerin fährt mit anderen Rollschuh.\n","trg:  Roller derby girl skating with others.\n","pred: During roller derby of other roller derby.\n","test set examples:\n","21\n","src:  Zwei Personen stehen neben einem Baum.\n","trg:  Two people standing next to a tree on the ground.\n","pred: Two people stand next to a tree.\n","src:  Eine Frau hält eine kleine weiße Statue.\n","trg:  A woman is holding a small white statue.\n","pred: A woman holds a small white statue.\n","src:  Ein Baby sitzt mit Lätzchen in einem Hochstuhl und isst einen Keks.\n","trg:  A baby girl eats a cookie while seated in a highchair and wearing a bib.\n","pred: A baby is sitting in a highchair with bib and eating a meal.\n","train_loss: 0.8141, valid_loss: 1.8296, test_loss: 1.8768\n","test_bleu: 35.5117, valid_bleu: 36.8139 train_bleu: 55.9213\n"]}]},{"cell_type":"markdown","source":["So with a transformer model of 26 million parameters, trained on a training set of 29k sentence pairs, we got a test BLEU score of about 35. The BLEU score seems pretty high, which I think it is because the sentences in this dataset are pretty short and simple. "],"metadata":{"id":"SE7RdQ57HN9J"}},{"cell_type":"code","source":["def translate_this_sentence(text: str):\n","    'translate the source sentence in string formate into target language'\n","    input = torch.tensor([[BOS] + tokenizers[SRC](text) + [EOS]]).to(DEVICE)\n","    output = translate(model, input)\n","    return decode_sentence(detokenizers[TRG], output[0])\n","\n","translate_this_sentence(\"Eine Gruppe von Menschen steht vor einem Iglu.\")"],"metadata":{"id":"6DHwo3xcYmXy","colab":{"base_uri":"https://localhost:8080/","height":38},"executionInfo":{"status":"ok","timestamp":1650937897984,"user_tz":240,"elapsed":223,"user":{"displayName":"Hongbin Chen","userId":"14161707569992931612"}},"outputId":"349e8712-520b-4df7-be58-06237de3e2d7"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'A group of people stand in front of an igloo.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]}]}